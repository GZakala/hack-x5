{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa58017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Загружаем данные (пример)\n",
    "df = pd.read_csv('../data/aug_train.csv', sep=';')\n",
    "df.columns = ['text', 'spans']\n",
    "df['spans'] = df['spans'].apply(lambda x: [{'start': span[0], 'end': span[1], 'label': span[2].replace('0', 'O')} for span in ast.literal_eval(x)])\n",
    "df = df.iloc[[i for i, s in enumerate(df['spans'].values) if s[0]['start'] == 0]]\n",
    "\n",
    "# словарь меток\n",
    "label2id = {\n",
    "    \"O\": 0, \n",
    "    \"B-BRAND\": 1, \"B-TYPE\": 2, \"B-VOLUME\": 3, \"B-PERCENT\": 4,\n",
    "    \"I-BRAND\": 5, \"I-TYPE\": 6, \"I-VOLUME\": 7, \"I-PERCENT\": 8,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Dataset\n",
    "# ---------------------------\n",
    "\n",
    "# model_name = \"cointegrated/rubert-tiny2\"\n",
    "# model_name = 'DeepPavlov/rubert-base-cased'\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# model_name = 'ai-forever/ruBert-large'\n",
    "model_name = 'ai-forever/ruRoberta-large'\n",
    "# model_name = \"sberbank-ai/ruBert-large\"\n",
    "# model_name = \"sberbank-ai/ruRoberta-large\"\n",
    "# model_name = 'ai-forever/FRED-T5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "MAX_LEN = 128\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, spans, tokenizer, label2id, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.spans = spans\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        spans = self.spans[idx]\n",
    "\n",
    "        # посимвольные метки\n",
    "        char_labels = [\"O\"] * len(text)\n",
    "        for span in spans:\n",
    "            start, end, tag = span[\"start\"], span[\"end\"], span[\"label\"]\n",
    "            char_labels[start] = tag\n",
    "            for i in range(start+1, end):\n",
    "                char_labels[i] = tag.replace(\"B-\", \"I-\")\n",
    "\n",
    "        # токенизация\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # метки на токенах\n",
    "        labels = []\n",
    "        numerics = []\n",
    "        offsets = enc[\"offset_mapping\"][0]\n",
    "        for start, end in offsets:\n",
    "            if start == end:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                labels.append(self.label2id[char_labels[start]] if start < len(char_labels) else -100)\n",
    "            \n",
    "            if text[start : end].isdigit():\n",
    "                numerics.append(1)\n",
    "            else:\n",
    "                numerics.append(0)\n",
    "\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items() if k != \"offset_mapping\"}\n",
    "        item[\"labels\"] = torch.tensor(labels)\n",
    "        item['is_numeric'] = torch.tensor(numerics)\n",
    "        return item\n",
    "\n",
    "\n",
    "# разделение train/test\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "full_dataset = NERDataset(df[\"text\"].tolist(), df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "train_dataset = NERDataset(train_df[\"text\"].tolist(), train_df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "test_dataset = NERDataset(test_df[\"text\"].tolist(), test_df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "\n",
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbd245da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "class NERLightning(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, num_labels: int, id2label: dict, label2id: dict, lr: float = 2e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Заморозим все слои, кроме последних 2\n",
    "        for _, param in self.encoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        n_unfreeze_layers = 2\n",
    "        if 'bert' in self.encoder.name_or_path.lower():\n",
    "            for layer in self.encoder.encoder.layer[-n_unfreeze_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        elif 't5' in self.encoder.name_or_path.lower():\n",
    "            self.encoder = self.encoder.encoder\n",
    "            for layer in self.encoder.block[-n_unfreeze_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        else:\n",
    "            raise TypeError(f'Invalid encoder name: {self.encoder.name_or_path}')\n",
    "\n",
    "        self.encoder_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Классификатор\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size + 1, num_labels)\n",
    "        torch.nn.init.xavier_uniform(self.classifier.weight)\n",
    "\n",
    "        # Взвешенный loss (меньше вес для класса \"O\")\n",
    "        class_weights = torch.ones(num_labels)\n",
    "        class_weights[label2id[\"O\"]] = 0.9\n",
    "        class_weights[label2id[\"B-TYPE\"]] = 0.2\n",
    "        class_weights[label2id[\"I-TYPE\"]] = 1.0\n",
    "        class_weights[label2id[\"B-BRAND\"]] = 0.35\n",
    "        class_weights[label2id[\"I-BRAND\"]] = 5.0\n",
    "        class_weights[label2id[\"B-PERCENT\"]] = 10.0   # было 15 → ограничили\n",
    "        class_weights[label2id[\"I-PERCENT\"]] = 10.0   # было 98 → ограничили\n",
    "        class_weights[label2id[\"B-VOLUME\"]] = 10.0    # было 41 → ограничили\n",
    "        class_weights[label2id[\"I-VOLUME\"]] = 10.0\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "        self.id2label = id2label\n",
    "        self.label2id = label2id\n",
    "        self.lr = lr\n",
    "\n",
    "        # Хранилище предсказаний для метрики\n",
    "        self.train_preds, self.train_labels = [], []\n",
    "        self.val_preds, self.val_labels = [], []\n",
    "\n",
    "        self.f1 = F1Score(task=\"multiclass\", num_classes=num_labels, average=\"macro\")\n",
    "\n",
    "    def forward(self, input_ids,  attention_mask, is_numeric, labels=None, **kwargs):\n",
    "        x = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.encoder_dropout(x.last_hidden_state)\n",
    "        x = torch.cat([x, is_numeric.unsqueeze(-1).float()], dim=-1)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, self.hparams.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss, logits = outputs[\"loss\"], outputs[\"logits\"]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().tolist()\n",
    "        labels = batch[\"labels\"].detach().cpu().tolist()\n",
    "\n",
    "        self.train_preds.extend(preds)\n",
    "        self.train_labels.extend(labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[\"loss\"], outputs[\"logits\"]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu()\n",
    "        labels = batch[\"labels\"].detach().cpu().tolist()\n",
    "\n",
    "        self.val_preds.extend(preds)\n",
    "        self.val_labels.extend(labels)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        f1 = self.compute_f1(self.train_labels, self.train_preds)\n",
    "        self.log(\"train_f1\", f1, prog_bar=True)\n",
    "        self.train_preds, self.train_labels = [], []  # очистка\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        f1 = self.compute_f1(self.val_labels, self.val_preds)\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "        self.val_preds, self.val_labels = [], []\n",
    "\n",
    "    def compute_f1(self, labels, preds):\n",
    "        # выравнивание + удаление -100\n",
    "        y_true, y_pred = [], []\n",
    "        for yt, yp in zip(labels, preds):\n",
    "            for t, p in zip(yt, yp):\n",
    "                if t == -100:\n",
    "                    continue\n",
    "                y_true.append(t)\n",
    "                y_pred.append(p)\n",
    "\n",
    "        return f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2069dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# === Кросс-валидация с сохранением модели ===\n",
    "def run_kfold_crossval(\n",
    "    dataset, \n",
    "    model_name: str, \n",
    "    num_labels: int,\n",
    "    label2id: dict[str, int],\n",
    "    id2label: dict[int, str],\n",
    "    batch_size=16, \n",
    "    lr=2e-5, \n",
    "    k=5, \n",
    "    max_epochs=5,\n",
    "    save_dir=\"../models\"\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\n===== Fold {fold+1} / {k} =====\")\n",
    "\n",
    "        # сабсеты\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "        # новая модель на каждом фолде\n",
    "        model = NERLightning(\n",
    "            model_name=model_name, \n",
    "            num_labels=num_labels, \n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "        # коллбэки\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=f\"{save_dir}/fold_{fold+1}\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            monitor=\"val_f1\",\n",
    "            mode=\"max\",\n",
    "        )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_f1\",\n",
    "            patience=1,\n",
    "            mode=\"max\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            devices=1,\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=10,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        # загрузка лучшей модели\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        print(f\"Best model saved at {best_model_path}\")\n",
    "\n",
    "        val_metrics = trainer.callback_metrics\n",
    "        results.append(val_metrics[\"val_f1\"].item())\n",
    "\n",
    "    avg_f1 = sum(results) / len(results)\n",
    "    print(f\"\\n===== Mean Macro-F1 across {k} folds: {avg_f1:.4f} =====\")\n",
    "    return results, avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb839380",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "DeepPavlov/rubert-large-cased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:407\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/DeepPavlov/rubert-large-cased/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:478\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1117\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1655\u001b[39m ):\n\u001b[32m   1656\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:457\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    448\u001b[39m     message = (\n\u001b[32m    449\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    456\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-68d40efa-50afd5711db573c0735d5007;d3ee2165-5e9b-4dda-bee2-a974e026fcb4)\n\nRepository Not Found for url: https://huggingface.co/DeepPavlov/rubert-large-cased/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mDeepPavlov/rubert-large-cased\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# model_name = 'xlm-roberta-base'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m full_dataset = NERDataset(df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].tolist(), df[\u001b[33m\"\u001b[39m\u001b[33mspans\u001b[39m\u001b[33m\"\u001b[39m].tolist(), tokenizer, label2id, MAX_LEN)\n\u001b[32m      7\u001b[39m run_kfold_crossval(\n\u001b[32m      8\u001b[39m     full_dataset, \n\u001b[32m      9\u001b[39m     model_name, \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     max_epochs=\u001b[32m10\u001b[39m,\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1058\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1057\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   1060\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:890\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    887\u001b[39m     token = use_auth_token\n\u001b[32m    889\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m890\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    907\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    264\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    265\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    266\u001b[39m     **kwargs,\n\u001b[32m    267\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:510\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    508\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    511\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    518\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    519\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    520\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    521\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: DeepPavlov/rubert-large-cased is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "# model_name = \"cointegrated/rubert-tiny2\"\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "# model_name = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "full_dataset = NERDataset(df[\"text\"].tolist(), df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "run_kfold_crossval(\n",
    "    full_dataset, \n",
    "    model_name, \n",
    "    num_labels=len(id2label),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4112/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /root/hack-x5/models/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval \n",
      "1 | encoder_dropout | Dropout           | 0      | train\n",
      "2 | classifier      | Linear            | 9.2 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | train\n",
      "4 | f1              | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 444 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 385/385 [01:30<00:00,  4.26it/s, v_num=5, train_loss=0.277, val_loss=0.356, val_f1=0.728]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 385/385 [01:31<00:00,  4.20it/s, v_num=5, train_loss=0.149, val_loss=0.257, val_f1=0.865, train_f1=0.663] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.137 >= min_delta = 0.0. New best score: 0.865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 385/385 [01:31<00:00,  4.20it/s, v_num=5, train_loss=0.192, val_loss=0.268, val_f1=0.866, train_f1=0.863] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.001 >= min_delta = 0.0. New best score: 0.866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 385/385 [01:30<00:00,  4.24it/s, v_num=5, train_loss=0.130, val_loss=0.298, val_f1=0.875, train_f1=0.926] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.009 >= min_delta = 0.0. New best score: 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 385/385 [01:30<00:00,  4.24it/s, v_num=5, train_loss=0.0223, val_loss=0.451, val_f1=0.873, train_f1=0.968] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_f1 did not improve in the last 2 records. Best score: 0.875. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 385/385 [01:30<00:00,  4.24it/s, v_num=5, train_loss=0.0223, val_loss=0.451, val_f1=0.873, train_f1=0.968]\n",
      "Best model saved at: /root/hack-x5/models/checkpoints/ai-forever/ruRoberta-large-ner-epoch=03-val_f1=0.8747.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4112/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_f1\",\n",
    "    patience=2,\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"../models/checkpoints\",\n",
    "    filename=model_name + \"-ner-{epoch:02d}-{val_f1:.4f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "model = NERLightning(\n",
    "    model_name=model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    lr=2e-4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"gpu\", devices=1, callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "best_model = NERLightning.load_from_checkpoint(\n",
    "    best_model_path, \n",
    "    num_labels=len(label2id), \n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "best_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2682fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4112/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode\n",
      "-------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval\n",
      "1 | encoder_dropout | Dropout           | 0      | eval\n",
      "2 | classifier      | Linear            | 9.2 K  | eval\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | eval\n",
      "4 | f1              | MulticlassF1Score | 0      | eval\n",
      "-------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "448       Modules in eval mode\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 449 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 428/428 [01:31<00:00,  4.69it/s, v_num=7, train_loss=0.0105, train_f1=0.933]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 428/428 [01:46<00:00,  4.03it/s, v_num=7, train_loss=0.0105, train_f1=0.933]\n"
     ]
    }
   ],
   "source": [
    "model = NERLightning(\n",
    "    model_name=model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    lr=2e-4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=4, accelerator=\"gpu\", devices=1)\n",
    "trainer.fit(model, full_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a030f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def save_model(model, save_dir=\"../models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1. веса модели\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"ner_model.bin\"))\n",
    "\n",
    "    # 2. метаданные\n",
    "    metadata = {\n",
    "        \"model_name\": model.hparams.model_name,\n",
    "        \"num_labels\": model.hparams.num_labels,\n",
    "        \"label2id\": model.hparams.label2id,\n",
    "        \"id2label\": model.hparams.id2label\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Model saved to {save_dir}\")\n",
    "\n",
    "def load_model(save_dir=\"../models\"):\n",
    "    # 1. метаданные\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # 2. инициализация модели\n",
    "    model = NERLightning(\n",
    "        model_name=metadata[\"model_name\"],\n",
    "        num_labels=metadata[\"num_labels\"],\n",
    "        id2label=metadata[\"id2label\"],\n",
    "        label2id=metadata[\"label2id\"]\n",
    "    )\n",
    "\n",
    "    # 3. загрузка весов\n",
    "    state_dict = torch.load(os.path.join(save_dir, \"ner_model.bin\"), map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # 4. токенайзер\n",
    "    tokenizer = AutoTokenizer.from_pretrained(metadata[\"model_name\"])\n",
    "\n",
    "    print(f\"✅ Model loaded from {save_dir}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "118a50f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to ../models/deeppavlov_ner_model\n"
     ]
    }
   ],
   "source": [
    "save_model(model, '../models/deeppavlov_ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2fc219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded from ../models\n"
     ]
    }
   ],
   "source": [
    "l_model, l_tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac0f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9951657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('сливки', (0, 6, 'B-TYPE')), ('13', (7, 9, 'B-PERCENT')), ('процентов', (10, 19, 'I-PERCENT'))]\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda')\n",
    "# пример использования\n",
    "def predict(text: str):\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True, padding=\"max_length\")\n",
    "    input_ids = enc[\"input_ids\"].to('cpu')\n",
    "    attention_mask = enc[\"attention_mask\"].to('cpu')\n",
    "    offsets = enc[\"offset_mapping\"][0]\n",
    "    is_numeric = []\n",
    "    for start, end in offsets:\n",
    "        if text[start : end].isdigit():\n",
    "            is_numeric.append(1)\n",
    "        else:\n",
    "            is_numeric.append(0)\n",
    "\n",
    "    is_numeric = torch.tensor([is_numeric])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=input_ids.to('cuda'),\n",
    "            attention_mask=attention_mask.to('cuda'),\n",
    "            is_numeric=is_numeric.to('cuda'),\n",
    "        )['logits'].argmax(dim=-1)[0].cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n",
    "\n",
    "    return logits, offsets, tokens\n",
    "\n",
    "def decode_predictions(text, offsets, labels):\n",
    "    tokens = text.split(' ')\n",
    "    token_offsets = []\n",
    "    cur_i = 0\n",
    "    for token in tokens:\n",
    "        start = cur_i\n",
    "        end = start + len(token)\n",
    "        token_offsets.append((start, end))\n",
    "        cur_i += len(token) + 1\n",
    "\n",
    "    bio_start_offsets = [int(o[0]) for o in offsets if o[0] != o[1]]\n",
    "    res = []\n",
    "    for token, (start, end) in zip(tokens, token_offsets):\n",
    "        idx_token_label = bio_start_offsets.index(start) + 1\n",
    "        label = labels[idx_token_label]\n",
    "        res.append((token, (start, end, id2label[label])))\n",
    "\n",
    "    return res\n",
    "\n",
    "text = \"сливки 13 процентов\"\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3311f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('корм', (0, 4, 'B-TYPE')), ('влажный', (5, 12, 'I-TYPE')), ('purina', (13, 19, 'B-BRAND')), ('one', (20, 23, 'I-BRAND'))]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text = \"корм влажный purina one\"\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cf4144ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бананаааа ['B-TYPE']\n",
      "[('бананаааа', (0, 9, 'B-TYPE'))]\n"
     ]
    }
   ],
   "source": [
    "row = test_df.sample(1).values[0]\n",
    "text = row[0]\n",
    "print(text, [s['label'] for s in row[1]])\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "773f1e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>форма для выпечки</td>\n",
       "      <td>[{'start': 0, 'end': 5, 'label': 'B-TYPE'}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>фарш свиной</td>\n",
       "      <td>[{'start': 0, 'end': 4, 'label': 'B-TYPE'}, {'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text                                              spans\n",
       "0  форма для выпечки  [{'start': 0, 'end': 5, 'label': 'B-TYPE'}, {'...\n",
       "1        фарш свиной  [{'start': 0, 'end': 4, 'label': 'B-TYPE'}, {'..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = pd.read_csv('../data/submission.csv', sep=';')\n",
    "sdf.columns = ['text', 'spans']\n",
    "sdf['spans'] = sdf['spans'].apply(lambda x: [{'start': span[0], 'end': span[1], 'label': span[2].replace('0', 'O')} for span in ast.literal_eval(x)])\n",
    "sdf = sdf.iloc[[i for i, s in enumerate(sdf['spans'].values) if s[0]['start'] == 0]]\n",
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13dca182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred: 100%|██████████| 4999/4999 [00:37<00:00, 132.33it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = sdf['text'].tolist()\n",
    "s_text_pred_bio = []\n",
    "for text in tqdm(texts, desc='Pred'):\n",
    "    logits, offsets, tokens = predict(text)\n",
    "    bio = decode_predictions(text, offsets, logits) \n",
    "    s_text_pred_bio.append(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "270667bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['spans'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sdf[\u001b[33m'\u001b[39m\u001b[33mannotation\u001b[39m\u001b[33m'\u001b[39m] = [[s[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m spans] \u001b[38;5;28;01mfor\u001b[39;00m spans \u001b[38;5;129;01min\u001b[39;00m s_text_pred_bio]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sdf = \u001b[43msdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspans\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m sdf.to_csv(\u001b[33m'\u001b[39m\u001b[33m../data/test.csv\u001b[39m\u001b[33m'\u001b[39m, sep=\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m sdf.head(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['spans'] not found in axis\""
     ]
    }
   ],
   "source": [
    "sdf['annotation'] = [[s[1] for s in spans] for spans in s_text_pred_bio]\n",
    "sdf = sdf.rename(columns={'text': 'sample'}).drop(columns='spans')\n",
    "sdf.to_csv('../data/test.csv', sep=';', index=False)\n",
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d9089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242009aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_macro_f1(y_true, y_pred, entity_types=(\"TYPE\",\"BRAND\",\"VOLUME\",\"PERCENT\")):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: списки предсказанных и эталонных сущностей для всех примеров\n",
    "    Каждое значение — список кортежей: (start, end, label)\n",
    "    label в формате 'B-TYPE', 'I-BRAND' и т.д.\n",
    "    \"\"\"\n",
    "    \n",
    "    # счётчики TP, FP, FN для каждого типа\n",
    "    stats = {etype: {\"TP\":0, \"FP\":0, \"FN\":0} for etype in entity_types}\n",
    "\n",
    "    for true_spans, pred_spans in zip(y_true, y_pred):\n",
    "        # создаём словари по типу сущности\n",
    "        true_by_type = defaultdict(list)\n",
    "        for start, end, label in true_spans:\n",
    "            etype = label.split(\"-\")[-1]\n",
    "            true_by_type[etype].append((start,end))\n",
    "        \n",
    "        pred_by_type = defaultdict(list)\n",
    "        for start, end, label in pred_spans:\n",
    "            etype = label.split(\"-\")[-1]\n",
    "            pred_by_type[etype].append((start,end))\n",
    "        \n",
    "        for etype in entity_types:\n",
    "            true_set = set(true_by_type.get(etype, []))\n",
    "            pred_set = set(pred_by_type.get(etype, []))\n",
    "            TP = len(true_set & pred_set)\n",
    "            FP = len(pred_set - true_set)\n",
    "            FN = len(true_set - pred_set)\n",
    "            stats[etype][\"TP\"] += TP\n",
    "            stats[etype][\"FP\"] += FP\n",
    "            stats[etype][\"FN\"] += FN\n",
    "\n",
    "    # вычисляем F1 для каждого типа\n",
    "    f1_scores = []\n",
    "    for etype in entity_types:\n",
    "        TP = stats[etype][\"TP\"]\n",
    "        FP = stats[etype][\"FP\"]\n",
    "        FN = stats[etype][\"FN\"]\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores)/len(f1_scores)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c23d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2738/2738 [01:17<00:00, 35.27it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = test_df['text'].tolist()\n",
    "text_pred_bio = []\n",
    "for text in tqdm(texts):\n",
    "    logits, offsets, tokens = predict(text)\n",
    "    bio = decode_predictions(text, offsets, logits) \n",
    "    text_pred_bio.append(bio)\n",
    "\n",
    "text_true_bio = [\n",
    "    [(span['start'], span['end'], span['label']) for span in spans]\n",
    "    for spans in test_df['spans'].tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "88d22de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>spansl</th>\n",
       "      <th>pred_spansl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>actimal</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>actimeuno</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text                                         spans  \\\n",
       "9     actimal  [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]   \n",
       "11  actimeuno  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]   \n",
       "\n",
       "                                      pred_spans             spansl  \\\n",
       "9   [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]  [(0, 7, B-BRAND)]   \n",
       "11  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]  [(0, 9, B-BRAND)]   \n",
       "\n",
       "          pred_spansl  \n",
       "9   [(0, 7, B-BRAND)]  \n",
       "11  [(0, 9, B-BRAND)]  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['pred_spans'] = [\n",
    "    [{'start': span[0], 'end': span[1], 'label': span[2]} for _, span in items] \n",
    "    for items in text_pred_bio\n",
    "]\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "30a8b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>spansl</th>\n",
       "      <th>pred_spansl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>actimal</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>actimeuno</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text                                         spans  \\\n",
       "9     actimal  [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]   \n",
       "11  actimeuno  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]   \n",
       "\n",
       "                                      pred_spans             spansl  \\\n",
       "9   [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]  [(0, 7, B-BRAND)]   \n",
       "11  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]  [(0, 9, B-BRAND)]   \n",
       "\n",
       "          pred_spansl  \n",
       "9   [(0, 7, B-BRAND)]  \n",
       "11  [(0, 9, B-BRAND)]  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['spansl'] = test_df['spans'].apply(lambda spans: [(s['start'], s['end'], s['label']) for s in spans])\n",
    "test_df['pred_spansl'] = test_df['pred_spans'].apply(lambda spans: [(s['start'], s['end'], s['label']) for s in spans])\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db97baa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spansl</th>\n",
       "      <th>pred_spansl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22823</th>\n",
       "      <td>сырокопченя кобаса</td>\n",
       "      <td>[(0, 11, O), (12, 18, O)]</td>\n",
       "      <td>[(0, 11, B-TYPE), (12, 18, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16163</th>\n",
       "      <td>перец черный горошком</td>\n",
       "      <td>[(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, O)]</td>\n",
       "      <td>[(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, I-T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11260</th>\n",
       "      <td>кэннон труcы</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 12, B-TYPE)]</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 12, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17625</th>\n",
       "      <td>приправа хмели сунели</td>\n",
       "      <td>[(0, 8, B-TYPE), (9, 14, I-TYPE), (15, 21, O)]</td>\n",
       "      <td>[(0, 8, B-TYPE), (9, 14, B-BRAND), (15, 21, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13584</th>\n",
       "      <td>мороженое фруктовый лед !</td>\n",
       "      <td>[(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, O)...</td>\n",
       "      <td>[(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14559</th>\n",
       "      <td>нпро</td>\n",
       "      <td>[(0, 4, O)]</td>\n",
       "      <td>[(0, 4, B-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23398</th>\n",
       "      <td>телятины охлжденне</td>\n",
       "      <td>[(0, 8, O), (9, 18, O)]</td>\n",
       "      <td>[(0, 8, B-TYPE), (9, 18, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10756</th>\n",
       "      <td>кртки, вероки</td>\n",
       "      <td>[(0, 6, O), (7, 13, O)]</td>\n",
       "      <td>[(0, 6, B-TYPE), (7, 13, I-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24300</th>\n",
       "      <td>удобреия, подкорма</td>\n",
       "      <td>[(0, 9, O), (10, 18, O)]</td>\n",
       "      <td>[(0, 9, B-TYPE), (10, 18, I-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25438</th>\n",
       "      <td>хрусteam</td>\n",
       "      <td>[(0, 8, B-BRAND)]</td>\n",
       "      <td>[(0, 8, B-TYPE)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text  \\\n",
       "22823         сырокопченя кобаса   \n",
       "16163      перец черный горошком   \n",
       "11260               кэннон труcы   \n",
       "17625      приправа хмели сунели   \n",
       "13584  мороженое фруктовый лед !   \n",
       "14559                       нпро   \n",
       "23398         телятины охлжденне   \n",
       "10756              кртки, вероки   \n",
       "24300         удобреия, подкорма   \n",
       "25438                   хрусteam   \n",
       "\n",
       "                                                  spansl  \\\n",
       "22823                          [(0, 11, O), (12, 18, O)]   \n",
       "16163     [(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, O)]   \n",
       "11260                 [(0, 6, B-BRAND), (7, 12, B-TYPE)]   \n",
       "17625     [(0, 8, B-TYPE), (9, 14, I-TYPE), (15, 21, O)]   \n",
       "13584  [(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, O)...   \n",
       "14559                                        [(0, 4, O)]   \n",
       "23398                            [(0, 8, O), (9, 18, O)]   \n",
       "10756                            [(0, 6, O), (7, 13, O)]   \n",
       "24300                           [(0, 9, O), (10, 18, O)]   \n",
       "25438                                  [(0, 8, B-BRAND)]   \n",
       "\n",
       "                                             pred_spansl  \n",
       "22823               [(0, 11, B-TYPE), (12, 18, B-BRAND)]  \n",
       "16163  [(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, I-T...  \n",
       "11260                [(0, 6, B-BRAND), (7, 12, B-BRAND)]  \n",
       "17625  [(0, 8, B-TYPE), (9, 14, B-BRAND), (15, 21, I-...  \n",
       "13584  [(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, I-...  \n",
       "14559                                   [(0, 4, B-TYPE)]  \n",
       "23398                 [(0, 8, B-TYPE), (9, 18, B-BRAND)]  \n",
       "10756                  [(0, 6, B-TYPE), (7, 13, I-TYPE)]  \n",
       "24300                 [(0, 9, B-TYPE), (10, 18, I-TYPE)]  \n",
       "25438                                   [(0, 8, B-TYPE)]  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['spansl'] != test_df['pred_spansl']].sample(10)[['text', 'spansl', 'pred_spansl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1f61470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'B-TYPE': 22183,\n",
       "         'B-BRAND': 6523,\n",
       "         'O': 4820,\n",
       "         'I-TYPE': 4109,\n",
       "         'I-BRAND': 438,\n",
       "         'B-PERCENT': 142,\n",
       "         'B-VOLUME': 53,\n",
       "         'I-VOLUME': 25,\n",
       "         'I-PERCENT': 22})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "c = Counter(chain(*[[s['label'] for s in spans] for spans in train_df['spans'].values]))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b6aa057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.8676535272656256\n"
     ]
    }
   ],
   "source": [
    "score = compute_macro_f1(text_true_bio, [[s[1] for s in spans] for spans in text_pred_bio])\n",
    "print(\"Macro F1:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73bd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df['text'].tolist()\n",
    "test_spans = [\n",
    "    [(s['start'], s['end'], s['label']) for s in spans]\n",
    "    for spans in test_df['spans'].tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5cd60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
