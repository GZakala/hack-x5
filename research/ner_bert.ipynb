{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa58017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–ø—Ä–∏–º–µ—Ä)\n",
    "df = pd.read_csv('../data/aug_train.csv', sep=';')\n",
    "df.columns = ['text', 'spans']\n",
    "df['spans'] = df['spans'].apply(lambda x: [{'start': span[0], 'end': span[1], 'label': span[2].replace('0', 'O')} for span in ast.literal_eval(x)])\n",
    "df = df.iloc[[i for i, s in enumerate(df['spans'].values) if s[0]['start'] == 0]]\n",
    "\n",
    "# —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫\n",
    "label2id = {\n",
    "    \"O\": 0, \n",
    "    \"B-BRAND\": 1, \"B-TYPE\": 2, \"B-VOLUME\": 3, \"B-PERCENT\": 4,\n",
    "    \"I-BRAND\": 5, \"I-TYPE\": 6, \"I-VOLUME\": 7, \"I-PERCENT\": 8,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Dataset\n",
    "# ---------------------------\n",
    "\n",
    "# model_name = \"cointegrated/rubert-tiny2\"\n",
    "# model_name = 'DeepPavlov/rubert-base-cased'\n",
    "model_name = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "MAX_LEN = 128\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, spans, tokenizer, label2id, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.spans = spans\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        spans = self.spans[idx]\n",
    "\n",
    "        # –ø–æ—Å–∏–º–≤–æ–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "        char_labels = [\"O\"] * len(text)\n",
    "        for span in spans:\n",
    "            start, end, tag = span[\"start\"], span[\"end\"], span[\"label\"]\n",
    "            char_labels[start] = tag\n",
    "            for i in range(start+1, end):\n",
    "                char_labels[i] = tag.replace(\"B-\", \"I-\")\n",
    "\n",
    "        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # –º–µ—Ç–∫–∏ –Ω–∞ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "        labels = []\n",
    "        numerics = []\n",
    "        offsets = enc[\"offset_mapping\"][0]\n",
    "        for start, end in offsets:\n",
    "            if start == end:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                labels.append(self.label2id[char_labels[start]] if start < len(char_labels) else -100)\n",
    "            \n",
    "            if text[start : end].isdigit():\n",
    "                numerics.append(1)\n",
    "            else:\n",
    "                numerics.append(0)\n",
    "\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items() if k != \"offset_mapping\"}\n",
    "        item[\"labels\"] = torch.tensor(labels)\n",
    "        item['is_numeric'] = torch.tensor(numerics)\n",
    "        return item\n",
    "\n",
    "\n",
    "# —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ train/test\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "full_dataset = NERDataset(df[\"text\"].tolist(), df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "train_dataset = NERDataset(train_df[\"text\"].tolist(), train_df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "test_dataset = NERDataset(test_df[\"text\"].tolist(), test_df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "\n",
    "full_loader = DataLoader(df, batch_size=16, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbd245da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "class NERLightning(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, num_labels: int, id2label: dict, label2id: dict, lr: float = 2e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # –ó–∞–º–æ—Ä–æ–∑–∏–º –≤—Å–µ —Å–ª–æ–∏, –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 2\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer in self.bert.encoder.layer[-1:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 1, num_labels)\n",
    "\n",
    "        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π loss (–º–µ–Ω—å—à–µ –≤–µ—Å –¥–ª—è –∫–ª–∞—Å—Å–∞ \"O\")\n",
    "        class_weights = torch.ones(num_labels)\n",
    "        class_weights[label2id[\"O\"]] = 0.1\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "        self.id2label = id2label\n",
    "        self.label2id = label2id\n",
    "        self.lr = lr\n",
    "\n",
    "        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏\n",
    "        self.train_preds, self.train_labels = [], []\n",
    "        self.val_preds, self.val_labels = [], []\n",
    "\n",
    "        self.f1 = F1Score(task=\"multiclass\", num_classes=num_labels, average=\"macro\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, is_numeric=None, labels=None, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if is_numeric is not None:\n",
    "            is_numeric = is_numeric.unsqueeze(-1).float()  # [B, L, 1]\n",
    "            sequence_output = torch.cat([sequence_output, is_numeric], dim=-1)\n",
    "\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, self.hparams.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss, logits = outputs[\"loss\"], outputs[\"logits\"]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().tolist()\n",
    "        labels = batch[\"labels\"].detach().cpu().tolist()\n",
    "\n",
    "        self.train_preds.extend(preds)\n",
    "        self.train_labels.extend(labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[\"loss\"], outputs[\"logits\"]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu()\n",
    "        labels = batch[\"labels\"].detach().cpu().tolist()\n",
    "\n",
    "        self.val_preds.extend(preds)\n",
    "        self.val_labels.extend(labels)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        f1 = self.compute_f1(self.train_labels, self.train_preds)\n",
    "        self.log(\"train_f1\", f1, prog_bar=True)\n",
    "        self.train_preds, self.train_labels = [], []  # –æ—á–∏—Å—Ç–∫–∞\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        f1 = self.compute_f1(self.val_labels, self.val_preds)\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "        self.val_preds, self.val_labels = [], []\n",
    "\n",
    "    def compute_f1(self, labels, preds):\n",
    "        # –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ + —É–¥–∞–ª–µ–Ω–∏–µ -100\n",
    "        y_true, y_pred = [], []\n",
    "        for yt, yp in zip(labels, preds):\n",
    "            for t, p in zip(yt, yp):\n",
    "                if t == -100:\n",
    "                    continue\n",
    "                y_true.append(t)\n",
    "                y_pred.append(p)\n",
    "\n",
    "        return f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd2069dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# === –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ ===\n",
    "def run_kfold_crossval(\n",
    "    dataset, \n",
    "    model_name: str, \n",
    "    num_labels: int,\n",
    "    label2id: dict[str, int],\n",
    "    id2label: dict[int, str],\n",
    "    batch_size=16, \n",
    "    lr=2e-5, \n",
    "    k=5, \n",
    "    max_epochs=5,\n",
    "    save_dir=\"../models\"\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\n===== Fold {fold+1} / {k} =====\")\n",
    "\n",
    "        # —Å–∞–±—Å–µ—Ç—ã\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "        # –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –∫–∞–∂–¥–æ–º —Ñ–æ–ª–¥–µ\n",
    "        model = NERLightning(\n",
    "            model_name=model_name, \n",
    "            num_labels=num_labels, \n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "        # –∫–æ–ª–ª–±—ç–∫–∏\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=f\"{save_dir}/fold_{fold+1}\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            monitor=\"val_f1\",\n",
    "            mode=\"max\",\n",
    "        )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_f1\",\n",
    "            patience=2,\n",
    "            mode=\"max\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            devices=1,\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=10,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        # –∑–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        print(f\"Best model saved at {best_model_path}\")\n",
    "\n",
    "        val_metrics = trainer.callback_metrics\n",
    "        results.append(val_metrics[\"val_f1\"].item())\n",
    "\n",
    "    avg_f1 = sum(results) / len(results)\n",
    "    print(f\"\\n===== Mean Macro-F1 across {k} folds: {avg_f1:.4f} =====\")\n",
    "    return results, avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb839380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | bert       | BertModel         | 29.2 M | eval \n",
      "1 | dropout    | Dropout           | 0      | train\n",
      "2 | classifier | Linear            | 2.8 K  | train\n",
      "3 | loss_fn    | CrossEntropyLoss  | 0      | train\n",
      "4 | f1         | MulticlassF1Score | 0      | train\n",
      "---------------------------------------------------------\n",
      "770 K     Trainable params\n",
      "28.4 M    Non-trainable params\n",
      "29.2 M    Total params\n",
      "116.786   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "66        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 66 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.88it/s, v_num=25, train_loss=0.200, val_loss=0.357, val_f1=0.443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.443\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.44328 (best 0.44328), saving model to '/root/hack-x5/models/fold_1/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|‚ñè         | 23/1369 [06:02<5:54:03,  0.06it/s, v_num=21, train_loss=1.720]val_loss=0.357, val_f1=0.443, train_f1=0.322] \n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 15.98it/s, v_num=25, train_loss=0.284, val_loss=0.285, val_f1=0.616, train_f1=0.322] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.173 >= min_delta = 0.0. New best score: 0.616\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.61595 (best 0.61595), saving model to '/root/hack-x5/models/fold_1/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:27<00:00, 15.73it/s, v_num=25, train_loss=0.307, val_loss=0.257, val_f1=0.684, train_f1=0.601] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.068 >= min_delta = 0.0. New best score: 0.684\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.68403 (best 0.68403), saving model to '/root/hack-x5/models/fold_1/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 15.95it/s, v_num=25, train_loss=0.178, val_loss=0.238, val_f1=0.729, train_f1=0.673] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.045 >= min_delta = 0.0. New best score: 0.729\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.72942 (best 0.72942), saving model to '/root/hack-x5/models/fold_1/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 15.96it/s, v_num=25, train_loss=0.279, val_loss=0.225, val_f1=0.774, train_f1=0.727] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.044 >= min_delta = 0.0. New best score: 0.774\n",
      "Epoch 4, global step 6845: 'val_f1' reached 0.77387 (best 0.77387), saving model to '/root/hack-x5/models/fold_1/best-checkpoint.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.90it/s, v_num=25, train_loss=0.279, val_loss=0.225, val_f1=0.774, train_f1=0.727]\n",
      "Best model saved at /root/hack-x5/models/fold_1/best-checkpoint.ckpt\n",
      "\n",
      "===== Fold 2 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | bert       | BertModel         | 29.2 M | eval \n",
      "1 | dropout    | Dropout           | 0      | train\n",
      "2 | classifier | Linear            | 2.8 K  | train\n",
      "3 | loss_fn    | CrossEntropyLoss  | 0      | train\n",
      "4 | f1         | MulticlassF1Score | 0      | train\n",
      "---------------------------------------------------------\n",
      "770 K     Trainable params\n",
      "28.4 M    Non-trainable params\n",
      "29.2 M    Total params\n",
      "116.786   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "66        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 66 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:28<00:00, 15.51it/s, v_num=26, train_loss=0.315, val_loss=0.353, val_f1=0.521]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.521\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.52098 (best 0.52098), saving model to '/root/hack-x5/models/fold_2/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.84it/s, v_num=26, train_loss=0.360, val_loss=0.280, val_f1=0.642, train_f1=0.322] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.121 >= min_delta = 0.0. New best score: 0.642\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.64247 (best 0.64247), saving model to '/root/hack-x5/models/fold_2/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 16.02it/s, v_num=26, train_loss=0.422, val_loss=0.250, val_f1=0.680, train_f1=0.622] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.038 >= min_delta = 0.0. New best score: 0.680\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.68042 (best 0.68042), saving model to '/root/hack-x5/models/fold_2/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:27<00:00, 15.69it/s, v_num=26, train_loss=0.168, val_loss=0.236, val_f1=0.733, train_f1=0.714] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.053 >= min_delta = 0.0. New best score: 0.733\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.73293 (best 0.73293), saving model to '/root/hack-x5/models/fold_2/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.74it/s, v_num=26, train_loss=0.201, val_loss=0.220, val_f1=0.758, train_f1=0.751] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.026 >= min_delta = 0.0. New best score: 0.758\n",
      "Epoch 4, global step 6845: 'val_f1' reached 0.75848 (best 0.75848), saving model to '/root/hack-x5/models/fold_2/best-checkpoint.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:27<00:00, 15.68it/s, v_num=26, train_loss=0.201, val_loss=0.220, val_f1=0.758, train_f1=0.751]\n",
      "Best model saved at /root/hack-x5/models/fold_2/best-checkpoint.ckpt\n",
      "\n",
      "===== Fold 3 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | bert       | BertModel         | 29.2 M | eval \n",
      "1 | dropout    | Dropout           | 0      | train\n",
      "2 | classifier | Linear            | 2.8 K  | train\n",
      "3 | loss_fn    | CrossEntropyLoss  | 0      | train\n",
      "4 | f1         | MulticlassF1Score | 0      | train\n",
      "---------------------------------------------------------\n",
      "770 K     Trainable params\n",
      "28.4 M    Non-trainable params\n",
      "29.2 M    Total params\n",
      "116.786   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "66        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 66 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.75it/s, v_num=27, train_loss=0.549, val_loss=0.352, val_f1=0.422]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.422\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.42167 (best 0.42167), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:28<00:00, 15.50it/s, v_num=27, train_loss=0.382, val_loss=0.284, val_f1=0.629, train_f1=0.314] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.207 >= min_delta = 0.0. New best score: 0.629\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.62883 (best 0.62883), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 16.03it/s, v_num=27, train_loss=0.600, val_loss=0.255, val_f1=0.760, train_f1=0.595] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.132 >= min_delta = 0.0. New best score: 0.760\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.76049 (best 0.76049), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.90it/s, v_num=27, train_loss=0.0905, val_loss=0.238, val_f1=0.812, train_f1=0.705]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.051 >= min_delta = 0.0. New best score: 0.812\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.81177 (best 0.81177), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:24<00:00, 16.23it/s, v_num=27, train_loss=0.143, val_loss=0.229, val_f1=0.809, train_f1=0.775] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 6845: 'val_f1' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:24<00:00, 16.21it/s, v_num=27, train_loss=0.143, val_loss=0.229, val_f1=0.809, train_f1=0.775]\n",
      "Best model saved at /root/hack-x5/models/fold_3/best-checkpoint.ckpt\n",
      "\n",
      "===== Fold 4 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | bert       | BertModel         | 29.2 M | eval \n",
      "1 | dropout    | Dropout           | 0      | train\n",
      "2 | classifier | Linear            | 2.8 K  | train\n",
      "3 | loss_fn    | CrossEntropyLoss  | 0      | train\n",
      "4 | f1         | MulticlassF1Score | 0      | train\n",
      "---------------------------------------------------------\n",
      "770 K     Trainable params\n",
      "28.4 M    Non-trainable params\n",
      "29.2 M    Total params\n",
      "116.786   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "66        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 66 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 15.95it/s, v_num=28, train_loss=0.230, val_loss=0.363, val_f1=0.413]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.413\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.41253 (best 0.41253), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.80it/s, v_num=28, train_loss=0.256, val_loss=0.293, val_f1=0.614, train_f1=0.318] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.201 >= min_delta = 0.0. New best score: 0.614\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.61357 (best 0.61357), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:38<00:00, 13.95it/s, v_num=28, train_loss=0.0532, val_loss=0.263, val_f1=0.706, train_f1=0.559]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.092 >= min_delta = 0.0. New best score: 0.706\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.70582 (best 0.70582), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:27<00:00, 15.65it/s, v_num=28, train_loss=0.373, val_loss=0.244, val_f1=0.749, train_f1=0.634] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.043 >= min_delta = 0.0. New best score: 0.749\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.74859 (best 0.74859), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.88it/s, v_num=28, train_loss=0.208, val_loss=0.232, val_f1=0.768, train_f1=0.731] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.019 >= min_delta = 0.0. New best score: 0.768\n",
      "Epoch 4, global step 6845: 'val_f1' reached 0.76775 (best 0.76775), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.82it/s, v_num=28, train_loss=0.208, val_loss=0.232, val_f1=0.768, train_f1=0.731]\n",
      "Best model saved at /root/hack-x5/models/fold_4/best-checkpoint.ckpt\n",
      "\n",
      "===== Fold 5 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | bert       | BertModel         | 29.2 M | eval \n",
      "1 | dropout    | Dropout           | 0      | train\n",
      "2 | classifier | Linear            | 2.8 K  | train\n",
      "3 | loss_fn    | CrossEntropyLoss  | 0      | train\n",
      "4 | f1         | MulticlassF1Score | 0      | train\n",
      "---------------------------------------------------------\n",
      "770 K     Trainable params\n",
      "28.4 M    Non-trainable params\n",
      "29.2 M    Total params\n",
      "116.786   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "66        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 66 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.76it/s, v_num=29, train_loss=0.211, val_loss=0.370, val_f1=0.476]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.476\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.47597 (best 0.47597), saving model to '/root/hack-x5/models/fold_5/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 16.10it/s, v_num=29, train_loss=0.304, val_loss=0.313, val_f1=0.603, train_f1=0.336] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.127 >= min_delta = 0.0. New best score: 0.603\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.60320 (best 0.60320), saving model to '/root/hack-x5/models/fold_5/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:25<00:00, 15.92it/s, v_num=29, train_loss=0.231, val_loss=0.277, val_f1=0.691, train_f1=0.587] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.087 >= min_delta = 0.0. New best score: 0.691\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.69067 (best 0.69067), saving model to '/root/hack-x5/models/fold_5/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.80it/s, v_num=29, train_loss=0.226, val_loss=0.258, val_f1=0.749, train_f1=0.681] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.058 >= min_delta = 0.0. New best score: 0.749\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.74871 (best 0.74871), saving model to '/root/hack-x5/models/fold_5/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:26<00:00, 15.74it/s, v_num=29, train_loss=0.136, val_loss=0.247, val_f1=0.778, train_f1=0.731] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.029 >= min_delta = 0.0. New best score: 0.778\n",
      "Epoch 4, global step 6845: 'val_f1' reached 0.77819 (best 0.77819), saving model to '/root/hack-x5/models/fold_5/best-checkpoint.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1369/1369 [01:27<00:00, 15.69it/s, v_num=29, train_loss=0.136, val_loss=0.247, val_f1=0.778, train_f1=0.731]\n",
      "Best model saved at /root/hack-x5/models/fold_5/best-checkpoint.ckpt\n",
      "\n",
      "===== Mean Macro-F1 across 5 folds: 0.7774 =====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7738693952560425,\n",
       "  0.7584832906723022,\n",
       "  0.8086845278739929,\n",
       "  0.7677515149116516,\n",
       "  0.7781853675842285],\n",
       " 0.7773948192596436)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "# model_name = 'DeepPavlov/rubert-base-cased'\n",
    "# model_name = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "full_dataset = NERDataset(df[\"text\"].tolist(), df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "run_kfold_crossval(\n",
    "    full_dataset, \n",
    "    model_name, \n",
    "    num_labels=len(id2label),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | bert       | XLMRobertaModel  | 278 M  | eval \n",
      "1 | dropout    | Dropout          | 0      | train\n",
      "2 | classifier | Linear           | 6.9 K  | train\n",
      "3 | loss_fn    | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "270 M     Non-trainable params\n",
      "278 M     Total params\n",
      "1,112.202 Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 228 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|‚ñå         | 88/1540 [00:57<15:52,  1.52it/s, v_num=17, train_loss=0.444]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = NERLightning(\n",
    "    model_name=model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    lr=2e-4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=4, accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a030f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def save_model(model, save_dir=\"../models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1. –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"ner_model.bin\"))\n",
    "\n",
    "    # 2. –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "    metadata = {\n",
    "        \"model_name\": model.hparams.model_name,\n",
    "        \"num_labels\": model.hparams.num_labels,\n",
    "        \"label2id\": model.hparams.label2id,\n",
    "        \"id2label\": model.hparams.id2label\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Model saved to {save_dir}\")\n",
    "\n",
    "def load_model(save_dir=\"../models\"):\n",
    "    # 1. –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # 2. –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "    model = NERLightning(\n",
    "        model_name=metadata[\"model_name\"],\n",
    "        num_labels=metadata[\"num_labels\"],\n",
    "        id2label=metadata[\"id2label\"],\n",
    "        label2id=metadata[\"label2id\"]\n",
    "    )\n",
    "\n",
    "    # 3. –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤\n",
    "    state_dict = torch.load(os.path.join(save_dir, \"ner_model.bin\"), map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # 4. —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
    "    tokenizer = AutoTokenizer.from_pretrained(metadata[\"model_name\"])\n",
    "\n",
    "    print(f\"‚úÖ Model loaded from {save_dir}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "118a50f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to ../models/deeppavlov_ner_model\n"
     ]
    }
   ],
   "source": [
    "save_model(model, '../models/deeppavlov_ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2fc219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded from ../models\n"
     ]
    }
   ],
   "source": [
    "l_model, l_tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55124bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9951657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('—Å–ª–∏–≤–∫–∏', (0, 6, 'B-TYPE')), ('13', (7, 9, 'B-PERCENT')), ('–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤', (10, 19, 'I-PERCENT'))]\n"
     ]
    }
   ],
   "source": [
    "# –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "def predict(text: str):\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True, padding=\"max_length\")\n",
    "    input_ids = enc[\"input_ids\"].to('cpu')\n",
    "    attention_mask = enc[\"attention_mask\"].to('cpu')\n",
    "    offsets = enc[\"offset_mapping\"][0]\n",
    "    is_numeric = []\n",
    "    for start, end in offsets:\n",
    "        if text[start : end].isdigit():\n",
    "            is_numeric.append(1)\n",
    "        else:\n",
    "            is_numeric.append(0)\n",
    "\n",
    "    is_numeric = torch.tensor([is_numeric])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, is_numeric=is_numeric)['logits'].argmax(dim=-1)[0].cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n",
    "\n",
    "    return logits, offsets, tokens\n",
    "\n",
    "def decode_predictions(text, offsets, labels):\n",
    "    tokens = text.split(' ')\n",
    "    token_offsets = []\n",
    "    cur_i = 0\n",
    "    for token in tokens:\n",
    "        start = cur_i\n",
    "        end = start + len(token)\n",
    "        token_offsets.append((start, end))\n",
    "        cur_i += len(token) + 1\n",
    "\n",
    "    bio_start_offsets = [int(o[0]) for o in offsets if o[0] != o[1]]\n",
    "    res = []\n",
    "    for token, (start, end) in zip(tokens, token_offsets):\n",
    "        idx_token_label = bio_start_offsets.index(start) + 1\n",
    "        label = labels[idx_token_label]\n",
    "        res.append((token, (start, end, id2label[label])))\n",
    "\n",
    "    return res\n",
    "\n",
    "text = \"—Å–ª–∏–≤–∫–∏ 13 –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤\"\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3311f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('–∫–æ—Ä–º', (0, 4, 'B-TYPE')), ('–≤–ª–∞–∂–Ω—ã–π', (5, 12, 'I-TYPE')), ('purina', (13, 19, 'B-BRAND')), ('one', (20, 23, 'I-BRAND'))]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text = \"–∫–æ—Ä–º –≤–ª–∞–∂–Ω—ã–π purina one\"\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf4144ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—è–±–ª–æ–∫–æ–≤ ['B-TYPE']\n",
      "[('—è–±–ª–æ–∫–æ–≤', (0, 7, 'B-TYPE'))]\n"
     ]
    }
   ],
   "source": [
    "row = test_df.sample(1).values[0]\n",
    "text = row[0]\n",
    "print(text, [s['label'] for s in row[1]])\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8eb6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_macro_f1(y_true, y_pred, entity_types=(\"TYPE\",\"BRAND\",\"VOLUME\",\"PERCENT\")):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: —Å–ø–∏—Å–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "    –ö–∞–∂–¥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π: (start, end, label)\n",
    "    label –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'B-TYPE', 'I-BRAND' –∏ —Ç.–¥.\n",
    "    \"\"\"\n",
    "    \n",
    "    # —Å—á—ë—Ç—á–∏–∫–∏ TP, FP, FN –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞\n",
    "    stats = {etype: {\"TP\":0, \"FP\":0, \"FN\":0} for etype in entity_types}\n",
    "\n",
    "    for true_spans, pred_spans in zip(y_true, y_pred):\n",
    "        # —Å–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä–∏ –ø–æ —Ç–∏–ø—É —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "        true_by_type = defaultdict(list)\n",
    "        for start, end, label in true_spans:\n",
    "            etype = label.split(\"-\")[-1]\n",
    "            true_by_type[etype].append((start,end))\n",
    "        \n",
    "        pred_by_type = defaultdict(list)\n",
    "        for start, end, label in pred_spans:\n",
    "            etype = label.split(\"-\")[-1]\n",
    "            pred_by_type[etype].append((start,end))\n",
    "        \n",
    "        for etype in entity_types:\n",
    "            true_set = set(true_by_type.get(etype, []))\n",
    "            pred_set = set(pred_by_type.get(etype, []))\n",
    "            TP = len(true_set & pred_set)\n",
    "            FP = len(pred_set - true_set)\n",
    "            FN = len(true_set - pred_set)\n",
    "            stats[etype][\"TP\"] += TP\n",
    "            stats[etype][\"FP\"] += FP\n",
    "            stats[etype][\"FN\"] += FN\n",
    "\n",
    "    # –≤—ã—á–∏—Å–ª—è–µ–º F1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞\n",
    "    f1_scores = []\n",
    "    for etype in entity_types:\n",
    "        TP = stats[etype][\"TP\"]\n",
    "        FP = stats[etype][\"FP\"]\n",
    "        FN = stats[etype][\"FN\"]\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores)/len(f1_scores)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59859215",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = test_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9e56a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|‚ñè         | 20/1533 [22:22<28:13:06,  0.01it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2725/2725 [03:34<00:00, 12.71it/s]\n"
     ]
    }
   ],
   "source": [
    "text_pred_bio = []\n",
    "for text in tqdm(texts):\n",
    "    logits, offsets, tokens = predict(text)\n",
    "    bio = decode_predictions(text, offsets, logits) \n",
    "    text_pred_bio.append(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb9e3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_true_bio = [\n",
    "    [(span['start'], span['end'], span['label']) for span in spans]\n",
    "    for spans in test_df['spans'].tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dabc800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.8704645610683013\n"
     ]
    }
   ],
   "source": [
    "score = compute_macro_f1(text_true_bio, [[s[1] for s in spans] for spans in text_pred_bio])\n",
    "print(\"Macro F1:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9de92a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df['text'].tolist()\n",
    "test_spans = [\n",
    "    [(s['start'], s['end'], s['label']) for s in spans]\n",
    "    for spans in test_df['spans'].tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "773f1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.read_csv('../data/submission.csv', sep=';')\n",
    "sdf.columns = ['text', 'spans']\n",
    "sdf['spans'] = sdf['spans'].apply(lambda x: [{'start': span[0], 'end': span[1], 'label': span[2].replace('0', 'O')} for span in ast.literal_eval(x)])\n",
    "sdf = sdf.iloc[[i for i, s in enumerate(sdf['spans'].values) if s[0]['start'] == 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53656581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>—Ñ–æ—Ä–º–∞ –¥–ª—è –≤—ã–ø–µ—á–∫–∏</td>\n",
       "      <td>[{'start': 0, 'end': 5, 'label': 'B-TYPE'}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—Ñ–∞—Ä—à —Å–≤–∏–Ω–æ–π</td>\n",
       "      <td>[{'start': 0, 'end': 4, 'label': 'B-TYPE'}, {'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text                                              spans\n",
       "0  —Ñ–æ—Ä–º–∞ –¥–ª—è –≤—ã–ø–µ—á–∫–∏  [{'start': 0, 'end': 5, 'label': 'B-TYPE'}, {'...\n",
       "1        —Ñ–∞—Ä—à —Å–≤–∏–Ω–æ–π  [{'start': 0, 'end': 4, 'label': 'B-TYPE'}, {'..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dca182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "texts = sdf['text'].tolist()\n",
    "s_text_pred_bio = []\n",
    "for text in tqdm(texts):\n",
    "    logits, offsets, tokens = predict(text)\n",
    "    bio = decode_predictions(text, offsets, logits) \n",
    "    s_text_pred_bio.append(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "270667bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbio = [[s[1] for s in spans] for spans in s_text_pred_bio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "085dedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['annotation'] = sbio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f961feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.rename(columns={'text': 'sample'}).drop(columns='spans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d461904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>—Ñ–æ—Ä–º–∞ –¥–ª—è –≤—ã–ø–µ—á–∫–∏</td>\n",
       "      <td>[(0, 5, B-TYPE), (6, 9, I-TYPE), (10, 17, I-TY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—Ñ–∞—Ä—à —Å–≤–∏–Ω–æ–π</td>\n",
       "      <td>[(0, 4, B-TYPE), (5, 11, I-TYPE)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sample                                         annotation\n",
       "0  —Ñ–æ—Ä–º–∞ –¥–ª—è –≤—ã–ø–µ—á–∫–∏  [(0, 5, B-TYPE), (6, 9, I-TYPE), (10, 17, I-TY...\n",
       "1        —Ñ–∞—Ä—à —Å–≤–∏–Ω–æ–π                  [(0, 4, B-TYPE), (5, 11, I-TYPE)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50647520",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.to_csv('../data/test.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d58283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5cd60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
