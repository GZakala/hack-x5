{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa58017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Загружаем данные (пример)\n",
    "df = pd.read_csv('../data/aug_train.csv', sep=';')\n",
    "df.columns = ['text', 'spans']\n",
    "df['spans'] = df['spans'].apply(lambda x: [{'start': span[0], 'end': span[1], 'label': span[2].replace('0', 'O')} for span in ast.literal_eval(x)])\n",
    "df = df.iloc[[i for i, s in enumerate(df['spans'].values) if s[0]['start'] == 0]]\n",
    "\n",
    "# словарь меток\n",
    "label2id = {\n",
    "    \"O\": 0, \n",
    "    \"B-BRAND\": 1, \"B-TYPE\": 2, \"B-VOLUME\": 3, \"B-PERCENT\": 4,\n",
    "    \"I-BRAND\": 5, \"I-TYPE\": 6, \"I-VOLUME\": 7, \"I-PERCENT\": 8,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Dataset\n",
    "# ---------------------------\n",
    "\n",
    "# model_name = \"cointegrated/rubert-tiny2\"\n",
    "# model_name = 'DeepPavlov/rubert-base-cased'\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# model_name = 'ai-forever/ruBert-large'\n",
    "model_name = 'ai-forever/ruRoberta-large'\n",
    "# model_name = \"sberbank-ai/ruBert-large\"\n",
    "# model_name = \"sberbank-ai/ruRoberta-large\"\n",
    "# model_name = 'ai-forever/FRED-T5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "MAX_LEN = 128\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, spans, tokenizer, label2id, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.spans = spans\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        spans = self.spans[idx]\n",
    "\n",
    "        # посимвольные метки\n",
    "        char_labels = [\"O\"] * len(text)\n",
    "        for span in spans:\n",
    "            start, end, tag = span[\"start\"], span[\"end\"], span[\"label\"]\n",
    "            char_labels[start] = tag\n",
    "            for i in range(start+1, end):\n",
    "                char_labels[i] = tag.replace(\"B-\", \"I-\")\n",
    "\n",
    "        # токенизация\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # метки на токенах\n",
    "        labels = []\n",
    "        numerics = []\n",
    "        offsets = enc[\"offset_mapping\"][0]\n",
    "        for start, end in offsets:\n",
    "            if start == end:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                labels.append(self.label2id[char_labels[start]] if start < len(char_labels) else -100)\n",
    "            \n",
    "            if text[start : end].isdigit():\n",
    "                numerics.append(1)\n",
    "            else:\n",
    "                numerics.append(0)\n",
    "\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items() if k != \"offset_mapping\"}\n",
    "        item[\"labels\"] = torch.tensor(labels)\n",
    "        item['is_numeric'] = torch.tensor(numerics)\n",
    "        return item\n",
    "\n",
    "\n",
    "# разделение train/test\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "full_dataset = NERDataset(df[\"text\"].tolist(), df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "train_dataset = NERDataset(train_df[\"text\"].tolist(), train_df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "test_dataset = NERDataset(test_df[\"text\"].tolist(), test_df[\"spans\"].tolist(), tokenizer, label2id, MAX_LEN)\n",
    "\n",
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd245da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "class NERLightning(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, num_labels: int, id2label: dict, label2id: dict, lr: float = 2e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Заморозим все слои, кроме последних 2\n",
    "        for _, param in self.encoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        n_unfreeze_layers = 2\n",
    "        if 'bert' in self.encoder.name_or_path.lower():\n",
    "            for layer in self.encoder.encoder.layer[-n_unfreeze_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        elif 't5' in self.encoder.name_or_path.lower():\n",
    "            self.encoder = self.encoder.encoder\n",
    "            for layer in self.encoder.block[-n_unfreeze_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        else:\n",
    "            raise TypeError(f'Invalid encoder name: {self.encoder.name_or_path}')\n",
    "\n",
    "        self.encoder_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Классификатор\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size + 1, num_labels)\n",
    "        torch.nn.init.xavier_uniform(self.classifier.weight)\n",
    "\n",
    "        # Взвешенный loss (меньше вес для класса \"O\")\n",
    "        class_weights = torch.ones(num_labels)\n",
    "        class_weights[label2id[\"O\"]] = 0.9\n",
    "        class_weights[label2id[\"B-TYPE\"]] = 0.2\n",
    "        class_weights[label2id[\"I-TYPE\"]] = 1.0\n",
    "        class_weights[label2id[\"B-BRAND\"]] = 0.35\n",
    "        class_weights[label2id[\"I-BRAND\"]] = 5.0\n",
    "        class_weights[label2id[\"B-PERCENT\"]] = 10.0   # было 15 → ограничили\n",
    "        class_weights[label2id[\"I-PERCENT\"]] = 10.0   # было 98 → ограничили\n",
    "        class_weights[label2id[\"B-VOLUME\"]] = 10.0    # было 41 → ограничили\n",
    "        class_weights[label2id[\"I-VOLUME\"]] = 10.0\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "        self.id2label = id2label\n",
    "        self.label2id = label2id\n",
    "        self.lr = lr\n",
    "\n",
    "        # Хранилище предсказаний для метрики\n",
    "        self.train_preds, self.train_labels = [], []\n",
    "        self.val_preds, self.val_labels = [], []\n",
    "\n",
    "        self.f1 = F1Score(task=\"multiclass\", num_classes=num_labels, average=\"macro\")\n",
    "\n",
    "    def forward(self, input_ids,  attention_mask, is_numeric, labels=None, **kwargs):\n",
    "        x = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.encoder_dropout(x.last_hidden_state)\n",
    "        x = torch.cat([x, is_numeric.unsqueeze(-1).float()], dim=-1)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, self.hparams.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss, logits = outputs[\"loss\"], outputs[\"logits\"]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().tolist()\n",
    "        labels = batch[\"labels\"].detach().cpu().tolist()\n",
    "\n",
    "        self.train_preds.extend(preds)\n",
    "        self.train_labels.extend(labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[\"loss\"], outputs[\"logits\"]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu()\n",
    "        labels = batch[\"labels\"].detach().cpu().tolist()\n",
    "\n",
    "        self.val_preds.extend(preds)\n",
    "        self.val_labels.extend(labels)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        f1 = self.compute_f1(self.train_labels, self.train_preds)\n",
    "        self.log(\"train_f1\", f1, prog_bar=True)\n",
    "        self.train_preds, self.train_labels = [], []  # очистка\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        f1 = self.compute_f1(self.val_labels, self.val_preds)\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "        self.val_preds, self.val_labels = [], []\n",
    "\n",
    "    def compute_f1(self, labels, preds):\n",
    "        # выравнивание + удаление -100\n",
    "        y_true, y_pred = [], []\n",
    "        for yt, yp in zip(labels, preds):\n",
    "            for t, p in zip(yt, yp):\n",
    "                if t == -100:\n",
    "                    continue\n",
    "                y_true.append(t)\n",
    "                y_pred.append(p)\n",
    "\n",
    "        return f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd2069dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# пример использования\n",
    "def predict(text: str, model):\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True, padding=\"max_length\")\n",
    "    input_ids = enc[\"input_ids\"].to('cpu')\n",
    "    attention_mask = enc[\"attention_mask\"].to('cpu')\n",
    "    offsets = enc[\"offset_mapping\"][0]\n",
    "    is_numeric = []\n",
    "    for start, end in offsets:\n",
    "        if text[start : end].isdigit():\n",
    "            is_numeric.append(1)\n",
    "        else:\n",
    "            is_numeric.append(0)\n",
    "\n",
    "    is_numeric = torch.tensor([is_numeric])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=input_ids.to('cuda'),\n",
    "            attention_mask=attention_mask.to('cuda'),\n",
    "            is_numeric=is_numeric.to('cuda'),\n",
    "        )['logits'].argmax(dim=-1)[0].cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n",
    "\n",
    "    return logits, offsets, tokens\n",
    "\n",
    "def decode_predictions(text, offsets, labels):\n",
    "    tokens = text.split(' ')\n",
    "    token_offsets = []\n",
    "    cur_i = 0\n",
    "    for token in tokens:\n",
    "        start = cur_i\n",
    "        end = start + len(token)\n",
    "        token_offsets.append((start, end))\n",
    "        cur_i += len(token) + 1\n",
    "\n",
    "    bio_start_offsets = [int(o[0]) for o in offsets if o[0] != o[1]]\n",
    "    res = []\n",
    "    for token, (start, end) in zip(tokens, token_offsets):\n",
    "        idx_token_label = bio_start_offsets.index(start) + 1\n",
    "        label = labels[idx_token_label]\n",
    "        res.append((start, end, id2label[label]))\n",
    "\n",
    "    return res\n",
    "\n",
    "# === Кросс-валидация с сохранением модели ===\n",
    "def run_kfold_crossval(\n",
    "    df: pd.DataFrame,\n",
    "    model_name: str,\n",
    "    num_labels: int,\n",
    "    label2id: dict[str, int],\n",
    "    id2label: dict[int, str],\n",
    "    batch_size=16,\n",
    "    lr=2e-5,\n",
    "    k=5,\n",
    "    max_epochs=5,\n",
    "    save_dir=\"../models\",\n",
    "    max_len: int = 128,\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    dataset = NERDataset(df[\"text\"].tolist(), df[\"spans\"].tolist(), tokenizer, label2id, max_len)\n",
    "    validates = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\n===== Fold {fold+1} / {k} =====\")\n",
    "\n",
    "        # сабсеты\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "        # новая модель на каждом фолде\n",
    "        model = NERLightning(\n",
    "            model_name=model_name, \n",
    "            num_labels=num_labels, \n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            lr=lr,\n",
    "        ).to('cuda')\n",
    "\n",
    "        # коллбэки\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=f\"{save_dir}/fold_{fold+1}\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            monitor=\"val_f1\",\n",
    "            mode=\"max\",\n",
    "        )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_f1\",\n",
    "            patience=1,\n",
    "            mode=\"max\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            devices=1,\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=10,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        val_metrics = trainer.callback_metrics\n",
    "        results.append(val_metrics[\"val_f1\"].item())\n",
    "\n",
    "        # загрузка лучшей модели\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        print(f\"Best model saved at {best_model_path}\")\n",
    "\n",
    "        best_model = NERLightning.load_from_checkpoint(\n",
    "            best_model_path, \n",
    "            num_labels=len(label2id), \n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "        )\n",
    "        best_model.eval().to('cuda')\n",
    "\n",
    "        for text, span in tqdm(df.iloc[val_idx, :].values, desc='Pred'):\n",
    "            logits, offsets, tokens = predict(text, best_model)\n",
    "            pred_span = decode_predictions(text, offsets, logits) \n",
    "            span = [(s['start'], s['end'], s['label']) for s in span]\n",
    "            validates.append((text, span, pred_span, fold))\n",
    "\n",
    "    avg_f1 = sum(results) / len(results)\n",
    "    print(f\"\\n===== Mean Macro-F1 across {k} folds: {avg_f1:.4f} =====\")\n",
    "    return results, validates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc7974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-PERCENT',\n",
       " 'I-VOLUME',\n",
       " 'B-VOLUME',\n",
       " 'B-PERCENT',\n",
       " 'I-BRAND',\n",
       " 'I-TYPE',\n",
       " 'O',\n",
       " 'B-BRAND',\n",
       " 'B-TYPE']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['spans'] \\\n",
    "    .apply(lambda x: [s['label'] for s in x]) \\\n",
    "    .explode() \\\n",
    "    .reset_index() \\\n",
    "    .drop(columns='index') \\\n",
    "    .groupby(['spans']).agg({'spans': 'count'}) \\\n",
    "    .rename(columns={'spans': 'count'}) \\\n",
    "    .reset_index() \\\n",
    "    .sort_values('count') \\\n",
    "    ['spans'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb839380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5684/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /root/hack-x5/models/fold_1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval \n",
      "1 | encoder_dropout | Dropout           | 0      | train\n",
      "2 | classifier      | Linear            | 9.2 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | train\n",
      "4 | f1              | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 444 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1369/1369 [02:02<00:00, 11.21it/s, v_num=8, train_loss=0.205, val_loss=0.336, val_f1=0.823]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.823\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.82345 (best 0.82345), saving model to '/root/hack-x5/models/fold_1/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1369/1369 [02:02<00:00, 11.15it/s, v_num=8, train_loss=0.219, val_loss=0.312, val_f1=0.874, train_f1=0.680] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.051 >= min_delta = 0.0. New best score: 0.874\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.87422 (best 0.87422), saving model to '/root/hack-x5/models/fold_1/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1369/1369 [02:02<00:00, 11.21it/s, v_num=8, train_loss=0.108, val_loss=0.288, val_f1=0.882, train_f1=0.875] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.008 >= min_delta = 0.0. New best score: 0.882\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.88229 (best 0.88229), saving model to '/root/hack-x5/models/fold_1/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1369/1369 [02:01<00:00, 11.23it/s, v_num=8, train_loss=0.0685, val_loss=0.373, val_f1=0.882, train_f1=0.913] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_f1 did not improve in the last 1 records. Best score: 0.882. Signaling Trainer to stop.\n",
      "Epoch 3, global step 5476: 'val_f1' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1369/1369 [02:02<00:00, 11.20it/s, v_num=8, train_loss=0.0685, val_loss=0.373, val_f1=0.882, train_f1=0.913]\n",
      "Best model saved at /root/hack-x5/models/fold_1/best-checkpoint-v4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5684/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "Pred:   0%|          | 0/5476 [00:00<?, ?it/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Pred: 100%|██████████| 5476/5476 [01:28<00:00, 61.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 2 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /root/hack-x5/models/fold_2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval \n",
      "1 | encoder_dropout | Dropout           | 0      | train\n",
      "2 | classifier      | Linear            | 9.2 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | train\n",
      "4 | f1              | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 444 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1369/1369 [02:00<00:00, 11.33it/s, v_num=9, train_loss=0.249, val_loss=0.322, val_f1=0.844]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.844\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.84368 (best 0.84368), saving model to '/root/hack-x5/models/fold_2/best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1369/1369 [02:01<00:00, 11.27it/s, v_num=9, train_loss=0.164, val_loss=0.275, val_f1=0.864, train_f1=0.684] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.020 >= min_delta = 0.0. New best score: 0.864\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.86410 (best 0.86410), saving model to '/root/hack-x5/models/fold_2/best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1369/1369 [02:00<00:00, 11.32it/s, v_num=9, train_loss=0.065, val_loss=0.290, val_f1=0.872, train_f1=0.867] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.008 >= min_delta = 0.0. New best score: 0.872\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.87219 (best 0.87219), saving model to '/root/hack-x5/models/fold_2/best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1369/1369 [02:00<00:00, 11.33it/s, v_num=9, train_loss=0.047, val_loss=0.308, val_f1=0.895, train_f1=0.909]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.023 >= min_delta = 0.0. New best score: 0.895\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.89507 (best 0.89507), saving model to '/root/hack-x5/models/fold_2/best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1369/1369 [02:00<00:00, 11.34it/s, v_num=9, train_loss=0.139, val_loss=0.404, val_f1=0.904, train_f1=0.932]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.009 >= min_delta = 0.0. New best score: 0.904\n",
      "Epoch 4, global step 6845: 'val_f1' reached 0.90445 (best 0.90445), saving model to '/root/hack-x5/models/fold_2/best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1369/1369 [02:01<00:00, 11.30it/s, v_num=9, train_loss=0.0105, val_loss=0.402, val_f1=0.909, train_f1=0.958]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.004 >= min_delta = 0.0. New best score: 0.909\n",
      "Epoch 5, global step 8214: 'val_f1' reached 0.90870 (best 0.90870), saving model to '/root/hack-x5/models/fold_2/best-checkpoint-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1369/1369 [02:01<00:00, 11.30it/s, v_num=9, train_loss=0.0629, val_loss=0.432, val_f1=0.905, train_f1=0.971] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_f1 did not improve in the last 1 records. Best score: 0.909. Signaling Trainer to stop.\n",
      "Epoch 6, global step 9583: 'val_f1' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1369/1369 [02:01<00:00, 11.27it/s, v_num=9, train_loss=0.0629, val_loss=0.432, val_f1=0.905, train_f1=0.971]\n",
      "Best model saved at /root/hack-x5/models/fold_2/best-checkpoint-v2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5684/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "Pred: 100%|██████████| 5476/5476 [01:30<00:00, 60.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 3 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval \n",
      "1 | encoder_dropout | Dropout           | 0      | train\n",
      "2 | classifier      | Linear            | 9.2 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | train\n",
      "4 | f1              | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 18.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 444 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1369/1369 [02:02<00:00, 11.21it/s, v_num=10, train_loss=0.188, val_loss=0.331, val_f1=0.808]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.808\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.80766 (best 0.80766), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1369/1369 [02:02<00:00, 11.18it/s, v_num=10, train_loss=0.143, val_loss=0.280, val_f1=0.850, train_f1=0.669] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.042 >= min_delta = 0.0. New best score: 0.850\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.84970 (best 0.84970), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1369/1369 [02:01<00:00, 11.27it/s, v_num=10, train_loss=0.0648, val_loss=0.266, val_f1=0.863, train_f1=0.868]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.014 >= min_delta = 0.0. New best score: 0.863\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.86322 (best 0.86322), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1369/1369 [02:01<00:00, 11.24it/s, v_num=10, train_loss=0.168, val_loss=0.336, val_f1=0.881, train_f1=0.911]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.018 >= min_delta = 0.0. New best score: 0.881\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.88143 (best 0.88143), saving model to '/root/hack-x5/models/fold_3/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1369/1369 [02:02<00:00, 11.16it/s, v_num=10, train_loss=0.0151, val_loss=0.354, val_f1=0.875, train_f1=0.943] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_f1 did not improve in the last 1 records. Best score: 0.881. Signaling Trainer to stop.\n",
      "Epoch 4, global step 6845: 'val_f1' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1369/1369 [02:03<00:00, 11.13it/s, v_num=10, train_loss=0.0151, val_loss=0.354, val_f1=0.875, train_f1=0.943]\n",
      "Best model saved at /root/hack-x5/models/fold_3/best-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5684/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "Pred: 100%|██████████| 5476/5476 [01:29<00:00, 61.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 4 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval \n",
      "1 | encoder_dropout | Dropout           | 0      | train\n",
      "2 | classifier      | Linear            | 9.2 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | train\n",
      "4 | f1              | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 19.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 444 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1369/1369 [02:01<00:00, 11.30it/s, v_num=11, train_loss=0.479, val_loss=0.304, val_f1=0.865]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.865\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.86500 (best 0.86500), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1369/1369 [02:02<00:00, 11.20it/s, v_num=11, train_loss=0.215, val_loss=0.293, val_f1=0.881, train_f1=0.673] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.016 >= min_delta = 0.0. New best score: 0.881\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.88136 (best 0.88136), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1369/1369 [02:04<00:00, 10.99it/s, v_num=11, train_loss=0.265, val_loss=0.264, val_f1=0.891, train_f1=0.849] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.010 >= min_delta = 0.0. New best score: 0.891\n",
      "Epoch 2, global step 4107: 'val_f1' reached 0.89086 (best 0.89086), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1369/1369 [02:03<00:00, 11.06it/s, v_num=11, train_loss=0.0203, val_loss=0.278, val_f1=0.901, train_f1=0.902] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.010 >= min_delta = 0.0. New best score: 0.901\n",
      "Epoch 3, global step 5476: 'val_f1' reached 0.90051 (best 0.90051), saving model to '/root/hack-x5/models/fold_4/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1369/1369 [02:01<00:00, 11.25it/s, v_num=11, train_loss=0.192, val_loss=0.364, val_f1=0.893, train_f1=0.935]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_f1 did not improve in the last 1 records. Best score: 0.901. Signaling Trainer to stop.\n",
      "Epoch 4, global step 6845: 'val_f1' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1369/1369 [02:02<00:00, 11.22it/s, v_num=11, train_loss=0.192, val_loss=0.364, val_f1=0.893, train_f1=0.935]\n",
      "Best model saved at /root/hack-x5/models/fold_4/best-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5684/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "Pred: 100%|██████████| 5475/5475 [01:30<00:00, 60.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 5 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval \n",
      "1 | encoder_dropout | Dropout           | 0      | train\n",
      "2 | classifier      | Linear            | 9.2 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | train\n",
      "4 | f1              | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 23.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 444 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1369/1369 [02:01<00:00, 11.25it/s, v_num=12, train_loss=0.162, val_loss=0.353, val_f1=0.832]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.832\n",
      "Epoch 0, global step 1369: 'val_f1' reached 0.83163 (best 0.83163), saving model to '/root/hack-x5/models/fold_5/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1369/1369 [02:01<00:00, 11.29it/s, v_num=12, train_loss=0.115, val_loss=0.310, val_f1=0.865, train_f1=0.689] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.033 >= min_delta = 0.0. New best score: 0.865\n",
      "Epoch 1, global step 2738: 'val_f1' reached 0.86506 (best 0.86506), saving model to '/root/hack-x5/models/fold_5/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1369/1369 [02:00<00:00, 11.35it/s, v_num=12, train_loss=0.303, val_loss=0.303, val_f1=0.862, train_f1=0.875] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_f1 did not improve in the last 1 records. Best score: 0.865. Signaling Trainer to stop.\n",
      "Epoch 2, global step 4107: 'val_f1' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1369/1369 [02:00<00:00, 11.32it/s, v_num=12, train_loss=0.303, val_loss=0.303, val_f1=0.862, train_f1=0.875]\n",
      "Best model saved at /root/hack-x5/models/fold_5/best-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5684/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "Pred: 100%|██████████| 5475/5475 [01:30<00:00, 60.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Mean Macro-F1 across 5 folds: 0.8833 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results, validate_rows = run_kfold_crossval(\n",
    "    df,\n",
    "    model_name=model_name, \n",
    "    num_labels=len(id2label),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    k=5,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45bb2ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>pspans</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abon</td>\n",
       "      <td>[(0, 4, 'O')]</td>\n",
       "      <td>[(0, 4, 'O')]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abtoys игрушк</td>\n",
       "      <td>[(0, 6, 'B-BRAND'), (7, 13, 'B-TYPE')]</td>\n",
       "      <td>[(0, 6, 'B-BRAND'), (7, 13, 'I-TYPE')]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text                                   spans  \\\n",
       "0           abon                           [(0, 4, 'O')]   \n",
       "1  abtoys игрушк  [(0, 6, 'B-BRAND'), (7, 13, 'B-TYPE')]   \n",
       "\n",
       "                                   pspans  fold  \n",
       "0                           [(0, 4, 'O')]     0  \n",
       "1  [(0, 6, 'B-BRAND'), (7, 13, 'I-TYPE')]     0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val_df = pd.DataFrame(validate_rows, columns=['text', 'spans', 'pspans', 'fold'])\n",
    "pred_val_df['spans'] = pred_val_df['spans'].apply(lambda x: str(x))\n",
    "pred_val_df['pspans'] = pred_val_df['pspans'].apply(lambda x: str(x))\n",
    "pred_val_df.to_csv('../data/pred_val_spans.csv', sep=';', index=False)\n",
    "pred_val_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e08269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>pspans</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abon</td>\n",
       "      <td>[(0, 4, O)]</td>\n",
       "      <td>[(0, 4, O)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abtoys игрушк</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 13, B-TYPE)]</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 13, I-TYPE)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>active</td>\n",
       "      <td>[(0, 6, B-BRAND)]</td>\n",
       "      <td>[(0, 6, B-BRAND)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agata</td>\n",
       "      <td>[(0, 5, B-BRAND)]</td>\n",
       "      <td>[(0, 5, B-BRAND)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agnesi пше</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 10, B-TYPE)]</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 10, B-TYPE)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27373</th>\n",
       "      <td>яыц</td>\n",
       "      <td>[(0, 3, B-TYPE)]</td>\n",
       "      <td>[(0, 3, B-TYPE)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27374</th>\n",
       "      <td>яыца</td>\n",
       "      <td>[(0, 4, B-TYPE)]</td>\n",
       "      <td>[(0, 4, B-TYPE)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27375</th>\n",
       "      <td>№1 газе</td>\n",
       "      <td>[(0, 2, B-BRAND), (3, 7, B-TYPE)]</td>\n",
       "      <td>[(0, 2, B-BRAND), (3, 7, B-BRAND)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27376</th>\n",
       "      <td>№1 кофейник</td>\n",
       "      <td>[(0, 2, B-BRAND), (3, 11, B-TYPE)]</td>\n",
       "      <td>[(0, 2, B-BRAND), (3, 11, I-TYPE)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27377</th>\n",
       "      <td>№1 са</td>\n",
       "      <td>[(0, 2, B-BRAND), (3, 5, B-TYPE)]</td>\n",
       "      <td>[(0, 2, I-TYPE), (3, 5, I-TYPE)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27378 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                text                               spans  \\\n",
       "0               abon                         [(0, 4, O)]   \n",
       "1      abtoys игрушк  [(0, 6, B-BRAND), (7, 13, B-TYPE)]   \n",
       "2             active                   [(0, 6, B-BRAND)]   \n",
       "3              agata                   [(0, 5, B-BRAND)]   \n",
       "4         agnesi пше  [(0, 6, B-BRAND), (7, 10, B-TYPE)]   \n",
       "...              ...                                 ...   \n",
       "27373            яыц                    [(0, 3, B-TYPE)]   \n",
       "27374           яыца                    [(0, 4, B-TYPE)]   \n",
       "27375        №1 газе   [(0, 2, B-BRAND), (3, 7, B-TYPE)]   \n",
       "27376    №1 кофейник  [(0, 2, B-BRAND), (3, 11, B-TYPE)]   \n",
       "27377          №1 са   [(0, 2, B-BRAND), (3, 5, B-TYPE)]   \n",
       "\n",
       "                                   pspans  fold  \n",
       "0                             [(0, 4, O)]     0  \n",
       "1      [(0, 6, B-BRAND), (7, 13, I-TYPE)]     0  \n",
       "2                       [(0, 6, B-BRAND)]     0  \n",
       "3                       [(0, 5, B-BRAND)]     0  \n",
       "4      [(0, 6, B-BRAND), (7, 10, B-TYPE)]     0  \n",
       "...                                   ...   ...  \n",
       "27373                    [(0, 3, B-TYPE)]     4  \n",
       "27374                    [(0, 4, B-TYPE)]     4  \n",
       "27375  [(0, 2, B-BRAND), (3, 7, B-BRAND)]     4  \n",
       "27376  [(0, 2, B-BRAND), (3, 11, I-TYPE)]     4  \n",
       "27377    [(0, 2, I-TYPE), (3, 5, I-TYPE)]     4  \n",
       "\n",
       "[27378 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val_df = pd.DataFrame(validate_rows, columns=['text', 'spans', 'pspans', 'fold'])\n",
    "pred_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e01b9fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>pspans</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>соус кисло</td>\n",
       "      <td>[(0, 4, B-TYPE), (5, 10, O)]</td>\n",
       "      <td>[(0, 4, B-TYPE), (5, 10, I-TYPE)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20412</th>\n",
       "      <td>сделай с</td>\n",
       "      <td>[(0, 6, B-TYPE), (7, 8, I-TYPE)]</td>\n",
       "      <td>[(0, 6, B-TYPE), (7, 8, O)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12777</th>\n",
       "      <td>киндер молочный ломтик</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 15, B-TYPE), (16, 22, I-...</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 15, I-TYPE), (16, 22, I-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13614</th>\n",
       "      <td>молоко молочная станция</td>\n",
       "      <td>[(0, 6, B-TYPE), (7, 15, B-BRAND), (16, 23, I-...</td>\n",
       "      <td>[(0, 6, B-TYPE), (7, 15, I-TYPE), (16, 23, I-B...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21056</th>\n",
       "      <td>тан царицы</td>\n",
       "      <td>[(0, 3, B-TYPE), (4, 10, B-BRAND)]</td>\n",
       "      <td>[(0, 3, B-TYPE), (4, 10, I-BRAND)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22007</th>\n",
       "      <td>crem</td>\n",
       "      <td>[(0, 4, B-TYPE)]</td>\n",
       "      <td>[(0, 4, B-BRAND)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>маркет мёд</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 10, B-TYPE)]</td>\n",
       "      <td>[(0, 6, O), (7, 10, B-TYPE)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6338</th>\n",
       "      <td>в маринаде</td>\n",
       "      <td>[(0, 1, B-TYPE), (2, 10, I-TYPE)]</td>\n",
       "      <td>[(0, 1, O), (2, 10, O)]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>русие колбсы</td>\n",
       "      <td>[(0, 5, O), (6, 12, O)]</td>\n",
       "      <td>[(0, 5, O), (6, 12, I-TYPE)]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25524</th>\n",
       "      <td>редбулл</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "      <td>[(0, 7, B-TYPE)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  \\\n",
       "4266                соус кисло   \n",
       "20412                 сделай с   \n",
       "12777   киндер молочный ломтик   \n",
       "13614  молоко молочная станция   \n",
       "21056               тан царицы   \n",
       "22007                     crem   \n",
       "2486                маркет мёд   \n",
       "6338                в маринаде   \n",
       "3773              русие колбсы   \n",
       "25524                  редбулл   \n",
       "\n",
       "                                                   spans  \\\n",
       "4266                        [(0, 4, B-TYPE), (5, 10, O)]   \n",
       "20412                   [(0, 6, B-TYPE), (7, 8, I-TYPE)]   \n",
       "12777  [(0, 6, B-BRAND), (7, 15, B-TYPE), (16, 22, I-...   \n",
       "13614  [(0, 6, B-TYPE), (7, 15, B-BRAND), (16, 23, I-...   \n",
       "21056                 [(0, 3, B-TYPE), (4, 10, B-BRAND)]   \n",
       "22007                                   [(0, 4, B-TYPE)]   \n",
       "2486                  [(0, 6, B-BRAND), (7, 10, B-TYPE)]   \n",
       "6338                   [(0, 1, B-TYPE), (2, 10, I-TYPE)]   \n",
       "3773                             [(0, 5, O), (6, 12, O)]   \n",
       "25524                                  [(0, 7, B-BRAND)]   \n",
       "\n",
       "                                                  pspans  fold  \n",
       "4266                   [(0, 4, B-TYPE), (5, 10, I-TYPE)]     0  \n",
       "20412                        [(0, 6, B-TYPE), (7, 8, O)]     3  \n",
       "12777  [(0, 6, B-BRAND), (7, 15, I-TYPE), (16, 22, I-...     2  \n",
       "13614  [(0, 6, B-TYPE), (7, 15, I-TYPE), (16, 23, I-B...     2  \n",
       "21056                 [(0, 3, B-TYPE), (4, 10, I-BRAND)]     3  \n",
       "22007                                  [(0, 4, B-BRAND)]     4  \n",
       "2486                        [(0, 6, O), (7, 10, B-TYPE)]     0  \n",
       "6338                             [(0, 1, O), (2, 10, O)]     1  \n",
       "3773                        [(0, 5, O), (6, 12, I-TYPE)]     0  \n",
       "25524                                   [(0, 7, B-TYPE)]     4  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val_df[pred_val_df['spans'] != pred_val_df['pspans']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38364e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: /root/models/fold/best_checkpoint.ckpt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/models/fold/best_checkpoint.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m best_model_path = Path.cwd().parent.parent / \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33mfold\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33mbest_checkpoint.ckpt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest model saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m best_model = \u001b[43mNERLightning\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m best_model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[39m, in \u001b[36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    122\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.method.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be called on an instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1662\u001b[39m, in \u001b[36mLightningModule.load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[39m\n\u001b[32m   1573\u001b[39m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[32m   1574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_checkpoint\u001b[39m(\n\u001b[32m   1575\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1580\u001b[39m     **kwargs: Any,\n\u001b[32m   1581\u001b[39m ) -> Self:\n\u001b[32m   1582\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[32m   1583\u001b[39m \u001b[33;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[32m   1584\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1660\u001b[39m \n\u001b[32m   1661\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1662\u001b[39m     loaded = \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1670\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:63\u001b[39m, in \u001b[36m_load_from_checkpoint\u001b[39m\u001b[34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m map_location = map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     checkpoint = \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[32m     66\u001b[39m checkpoint = _pl_migrate_checkpoint(\n\u001b[32m     67\u001b[39m     checkpoint, checkpoint_path=(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     68\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/lightning_fabric/utilities/cloud_io.py:60\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(path_or_url, map_location, weights_only)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.hub.load_state_dict_from_url(\n\u001b[32m     55\u001b[39m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[32m     56\u001b[39m         map_location=map_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     57\u001b[39m         weights_only=weights_only,\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m fs = get_filesystem(path_or_url)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.load(\n\u001b[32m     62\u001b[39m         f,\n\u001b[32m     63\u001b[39m         map_location=map_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     64\u001b[39m         weights_only=weights_only,\n\u001b[32m     65\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/fsspec/spec.py:1338\u001b[39m, in \u001b[36mAbstractFileSystem.open\u001b[39m\u001b[34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[39m\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1337\u001b[39m     ac = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mautocommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._intrans)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1347\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/fsspec/implementations/local.py:210\u001b[39m, in \u001b[36mLocalFileSystem._open\u001b[39m\u001b[34m(self, path, mode, block_size, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedirs(\u001b[38;5;28mself\u001b[39m._parent(path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/fsspec/implementations/local.py:387\u001b[39m, in \u001b[36mLocalFileOpener.__init__\u001b[39m\u001b[34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28mself\u001b[39m.compression = get_compression(path, compression)\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m.blocksize = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/fsspec/implementations/local.py:392\u001b[39m, in \u001b[36mLocalFileOpener._open\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f.closed:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode:\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m         \u001b[38;5;28mself\u001b[39m.f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compression:\n\u001b[32m    394\u001b[39m             compress = compr[\u001b[38;5;28mself\u001b[39m.compression]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/root/models/fold/best_checkpoint.ckpt'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "best_model_path = Path.cwd().parent / 'models' / 'fold' / 'best_checkpoint.ckpt'\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "best_model = NERLightning.load_from_checkpoint(\n",
    "    best_model_path, \n",
    "    num_labels=len(label2id), \n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0aff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af653b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4112/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /root/hack-x5/models/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval \n",
      "1 | encoder_dropout | Dropout           | 0      | train\n",
      "2 | classifier      | Linear            | 9.2 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | train\n",
      "4 | f1              | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 444 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 385/385 [01:30<00:00,  4.26it/s, v_num=5, train_loss=0.277, val_loss=0.356, val_f1=0.728]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 385/385 [01:31<00:00,  4.20it/s, v_num=5, train_loss=0.149, val_loss=0.257, val_f1=0.865, train_f1=0.663] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.137 >= min_delta = 0.0. New best score: 0.865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 385/385 [01:31<00:00,  4.20it/s, v_num=5, train_loss=0.192, val_loss=0.268, val_f1=0.866, train_f1=0.863] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.001 >= min_delta = 0.0. New best score: 0.866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 385/385 [01:30<00:00,  4.24it/s, v_num=5, train_loss=0.130, val_loss=0.298, val_f1=0.875, train_f1=0.926] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.009 >= min_delta = 0.0. New best score: 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 385/385 [01:30<00:00,  4.24it/s, v_num=5, train_loss=0.0223, val_loss=0.451, val_f1=0.873, train_f1=0.968] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_f1 did not improve in the last 2 records. Best score: 0.875. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 385/385 [01:30<00:00,  4.24it/s, v_num=5, train_loss=0.0223, val_loss=0.451, val_f1=0.873, train_f1=0.968]\n",
      "Best model saved at: /root/hack-x5/models/checkpoints/ai-forever/ruRoberta-large-ner-epoch=03-val_f1=0.8747.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4112/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_f1\",\n",
    "    patience=2,\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"../models/checkpoints\",\n",
    "    filename=model_name + \"-ner-{epoch:02d}-{val_f1:.4f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "model = NERLightning(\n",
    "    model_name=model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    lr=2e-4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"gpu\", devices=1, callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "best_model = NERLightning.load_from_checkpoint(\n",
    "    best_model_path, \n",
    "    num_labels=len(label2id), \n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "best_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2682fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4112/3148999956.py:36: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode\n",
      "-------------------------------------------------------------\n",
      "0 | encoder         | RobertaModel      | 355 M  | eval\n",
      "1 | encoder_dropout | Dropout           | 0      | eval\n",
      "2 | classifier      | Linear            | 9.2 K  | eval\n",
      "3 | loss_fn         | CrossEntropyLoss  | 0      | eval\n",
      "4 | f1              | MulticlassF1Score | 0      | eval\n",
      "-------------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "330 M     Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.476 Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "448       Modules in eval mode\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/root/hack-x5/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 449 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 428/428 [01:31<00:00,  4.69it/s, v_num=7, train_loss=0.0105, train_f1=0.933]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 428/428 [01:46<00:00,  4.03it/s, v_num=7, train_loss=0.0105, train_f1=0.933]\n"
     ]
    }
   ],
   "source": [
    "model = NERLightning(\n",
    "    model_name=model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    lr=2e-4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=4, accelerator=\"gpu\", devices=1)\n",
    "trainer.fit(model, full_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a030f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def save_model(model, save_dir=\"../models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1. веса модели\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"ner_model.bin\"))\n",
    "\n",
    "    # 2. метаданные\n",
    "    metadata = {\n",
    "        \"model_name\": model.hparams.model_name,\n",
    "        \"num_labels\": model.hparams.num_labels,\n",
    "        \"label2id\": model.hparams.label2id,\n",
    "        \"id2label\": model.hparams.id2label\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Model saved to {save_dir}\")\n",
    "\n",
    "def load_model(save_dir=\"../models\"):\n",
    "    # 1. метаданные\n",
    "    with open(os.path.join(save_dir, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # 2. инициализация модели\n",
    "    model = NERLightning(\n",
    "        model_name=metadata[\"model_name\"],\n",
    "        num_labels=metadata[\"num_labels\"],\n",
    "        id2label=metadata[\"id2label\"],\n",
    "        label2id=metadata[\"label2id\"]\n",
    "    )\n",
    "\n",
    "    # 3. загрузка весов\n",
    "    state_dict = torch.load(os.path.join(save_dir, \"ner_model.bin\"), map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # 4. токенайзер\n",
    "    tokenizer = AutoTokenizer.from_pretrained(metadata[\"model_name\"])\n",
    "\n",
    "    print(f\"✅ Model loaded from {save_dir}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "118a50f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to ../models/deeppavlov_ner_model\n"
     ]
    }
   ],
   "source": [
    "save_model(model, '../models/deeppavlov_ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2fc219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded from ../models\n"
     ]
    }
   ],
   "source": [
    "l_model, l_tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9951657",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mmodel\u001b[49m.to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# пример использования\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# пример использования\n",
    "def predict(text: str):\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True, padding=\"max_length\")\n",
    "    input_ids = enc[\"input_ids\"].to('cpu')\n",
    "    attention_mask = enc[\"attention_mask\"].to('cpu')\n",
    "    offsets = enc[\"offset_mapping\"][0]\n",
    "    is_numeric = []\n",
    "    for start, end in offsets:\n",
    "        if text[start : end].isdigit():\n",
    "            is_numeric.append(1)\n",
    "        else:\n",
    "            is_numeric.append(0)\n",
    "\n",
    "    is_numeric = torch.tensor([is_numeric])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=input_ids.to('cuda'),\n",
    "            attention_mask=attention_mask.to('cuda'),\n",
    "            is_numeric=is_numeric.to('cuda'),\n",
    "        )['logits'].argmax(dim=-1)[0].cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n",
    "\n",
    "    return logits, offsets, tokens\n",
    "\n",
    "def decode_predictions(text, offsets, labels):\n",
    "    tokens = text.split(' ')\n",
    "    token_offsets = []\n",
    "    cur_i = 0\n",
    "    for token in tokens:\n",
    "        start = cur_i\n",
    "        end = start + len(token)\n",
    "        token_offsets.append((start, end))\n",
    "        cur_i += len(token) + 1\n",
    "\n",
    "    bio_start_offsets = [int(o[0]) for o in offsets if o[0] != o[1]]\n",
    "    res = []\n",
    "    for token, (start, end) in zip(tokens, token_offsets):\n",
    "        idx_token_label = bio_start_offsets.index(start) + 1\n",
    "        label = labels[idx_token_label]\n",
    "        res.append((token, (start, end, id2label[label])))\n",
    "\n",
    "    return res\n",
    "\n",
    "text = \"сливки 13 процентов\"\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3311f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('корм', (0, 4, 'B-TYPE')), ('влажный', (5, 12, 'I-TYPE')), ('purina', (13, 19, 'B-BRAND')), ('one', (20, 23, 'I-BRAND'))]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "text = \"корм влажный purina one\"\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cf4144ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бананаааа ['B-TYPE']\n",
      "[('бананаааа', (0, 9, 'B-TYPE'))]\n"
     ]
    }
   ],
   "source": [
    "row = test_df.sample(1).values[0]\n",
    "text = row[0]\n",
    "print(text, [s['label'] for s in row[1]])\n",
    "logits, offsets, tokens = predict(text)\n",
    "print(decode_predictions(text, offsets, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "773f1e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>форма для выпечки</td>\n",
       "      <td>[{'start': 0, 'end': 5, 'label': 'B-TYPE'}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>фарш свиной</td>\n",
       "      <td>[{'start': 0, 'end': 4, 'label': 'B-TYPE'}, {'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text                                              spans\n",
       "0  форма для выпечки  [{'start': 0, 'end': 5, 'label': 'B-TYPE'}, {'...\n",
       "1        фарш свиной  [{'start': 0, 'end': 4, 'label': 'B-TYPE'}, {'..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = pd.read_csv('../data/submission.csv', sep=';')\n",
    "sdf.columns = ['text', 'spans']\n",
    "sdf['spans'] = sdf['spans'].apply(lambda x: [{'start': span[0], 'end': span[1], 'label': span[2].replace('0', 'O')} for span in ast.literal_eval(x)])\n",
    "sdf = sdf.iloc[[i for i, s in enumerate(sdf['spans'].values) if s[0]['start'] == 0]]\n",
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13dca182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred: 100%|██████████| 4999/4999 [00:37<00:00, 132.33it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = sdf['text'].tolist()\n",
    "s_text_pred_bio = []\n",
    "for text in tqdm(texts, desc='Pred'):\n",
    "    logits, offsets, tokens = predict(text)\n",
    "    bio = decode_predictions(text, offsets, logits) \n",
    "    s_text_pred_bio.append(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "270667bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['spans'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sdf[\u001b[33m'\u001b[39m\u001b[33mannotation\u001b[39m\u001b[33m'\u001b[39m] = [[s[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m spans] \u001b[38;5;28;01mfor\u001b[39;00m spans \u001b[38;5;129;01min\u001b[39;00m s_text_pred_bio]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sdf = \u001b[43msdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspans\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m sdf.to_csv(\u001b[33m'\u001b[39m\u001b[33m../data/test.csv\u001b[39m\u001b[33m'\u001b[39m, sep=\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m sdf.head(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack-x5/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['spans'] not found in axis\""
     ]
    }
   ],
   "source": [
    "sdf['annotation'] = [[s[1] for s in spans] for spans in s_text_pred_bio]\n",
    "sdf = sdf.rename(columns={'text': 'sample'}).drop(columns='spans')\n",
    "sdf.to_csv('../data/test.csv', sep=';', index=False)\n",
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d9089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242009aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_macro_f1(y_true, y_pred, entity_types=(\"TYPE\",\"BRAND\",\"VOLUME\",\"PERCENT\")):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: списки предсказанных и эталонных сущностей для всех примеров\n",
    "    Каждое значение — список кортежей: (start, end, label)\n",
    "    label в формате 'B-TYPE', 'I-BRAND' и т.д.\n",
    "    \"\"\"\n",
    "    \n",
    "    # счётчики TP, FP, FN для каждого типа\n",
    "    stats = {etype: {\"TP\":0, \"FP\":0, \"FN\":0} for etype in entity_types}\n",
    "\n",
    "    for true_spans, pred_spans in zip(y_true, y_pred):\n",
    "        # создаём словари по типу сущности\n",
    "        true_by_type = defaultdict(list)\n",
    "        for start, end, label in true_spans:\n",
    "            etype = label.split(\"-\")[-1]\n",
    "            true_by_type[etype].append((start,end))\n",
    "        \n",
    "        pred_by_type = defaultdict(list)\n",
    "        for start, end, label in pred_spans:\n",
    "            etype = label.split(\"-\")[-1]\n",
    "            pred_by_type[etype].append((start,end))\n",
    "        \n",
    "        for etype in entity_types:\n",
    "            true_set = set(true_by_type.get(etype, []))\n",
    "            pred_set = set(pred_by_type.get(etype, []))\n",
    "            TP = len(true_set & pred_set)\n",
    "            FP = len(pred_set - true_set)\n",
    "            FN = len(true_set - pred_set)\n",
    "            stats[etype][\"TP\"] += TP\n",
    "            stats[etype][\"FP\"] += FP\n",
    "            stats[etype][\"FN\"] += FN\n",
    "\n",
    "    # вычисляем F1 для каждого типа\n",
    "    f1_scores = []\n",
    "    for etype in entity_types:\n",
    "        TP = stats[etype][\"TP\"]\n",
    "        FP = stats[etype][\"FP\"]\n",
    "        FN = stats[etype][\"FN\"]\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores)/len(f1_scores)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c23d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2738/2738 [01:17<00:00, 35.27it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = test_df['text'].tolist()\n",
    "text_pred_bio = []\n",
    "for text in tqdm(texts):\n",
    "    logits, offsets, tokens = predict(text)\n",
    "    bio = decode_predictions(text, offsets, logits) \n",
    "    text_pred_bio.append(bio)\n",
    "\n",
    "text_true_bio = [\n",
    "    [(span['start'], span['end'], span['label']) for span in spans]\n",
    "    for spans in test_df['spans'].tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "88d22de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>spansl</th>\n",
       "      <th>pred_spansl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>actimal</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>actimeuno</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text                                         spans  \\\n",
       "9     actimal  [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]   \n",
       "11  actimeuno  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]   \n",
       "\n",
       "                                      pred_spans             spansl  \\\n",
       "9   [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]  [(0, 7, B-BRAND)]   \n",
       "11  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]  [(0, 9, B-BRAND)]   \n",
       "\n",
       "          pred_spansl  \n",
       "9   [(0, 7, B-BRAND)]  \n",
       "11  [(0, 9, B-BRAND)]  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['pred_spans'] = [\n",
    "    [{'start': span[0], 'end': span[1], 'label': span[2]} for _, span in items] \n",
    "    for items in text_pred_bio\n",
    "]\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "30a8b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spans</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>spansl</th>\n",
       "      <th>pred_spansl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>actimal</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>actimeuno</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'label': 'B-BRAND'}]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "      <td>[(0, 9, B-BRAND)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text                                         spans  \\\n",
       "9     actimal  [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]   \n",
       "11  actimeuno  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]   \n",
       "\n",
       "                                      pred_spans             spansl  \\\n",
       "9   [{'start': 0, 'end': 7, 'label': 'B-BRAND'}]  [(0, 7, B-BRAND)]   \n",
       "11  [{'start': 0, 'end': 9, 'label': 'B-BRAND'}]  [(0, 9, B-BRAND)]   \n",
       "\n",
       "          pred_spansl  \n",
       "9   [(0, 7, B-BRAND)]  \n",
       "11  [(0, 9, B-BRAND)]  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['spansl'] = test_df['spans'].apply(lambda spans: [(s['start'], s['end'], s['label']) for s in spans])\n",
    "test_df['pred_spansl'] = test_df['pred_spans'].apply(lambda spans: [(s['start'], s['end'], s['label']) for s in spans])\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db97baa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spansl</th>\n",
       "      <th>pred_spansl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22823</th>\n",
       "      <td>сырокопченя кобаса</td>\n",
       "      <td>[(0, 11, O), (12, 18, O)]</td>\n",
       "      <td>[(0, 11, B-TYPE), (12, 18, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16163</th>\n",
       "      <td>перец черный горошком</td>\n",
       "      <td>[(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, O)]</td>\n",
       "      <td>[(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, I-T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11260</th>\n",
       "      <td>кэннон труcы</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 12, B-TYPE)]</td>\n",
       "      <td>[(0, 6, B-BRAND), (7, 12, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17625</th>\n",
       "      <td>приправа хмели сунели</td>\n",
       "      <td>[(0, 8, B-TYPE), (9, 14, I-TYPE), (15, 21, O)]</td>\n",
       "      <td>[(0, 8, B-TYPE), (9, 14, B-BRAND), (15, 21, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13584</th>\n",
       "      <td>мороженое фруктовый лед !</td>\n",
       "      <td>[(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, O)...</td>\n",
       "      <td>[(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14559</th>\n",
       "      <td>нпро</td>\n",
       "      <td>[(0, 4, O)]</td>\n",
       "      <td>[(0, 4, B-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23398</th>\n",
       "      <td>телятины охлжденне</td>\n",
       "      <td>[(0, 8, O), (9, 18, O)]</td>\n",
       "      <td>[(0, 8, B-TYPE), (9, 18, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10756</th>\n",
       "      <td>кртки, вероки</td>\n",
       "      <td>[(0, 6, O), (7, 13, O)]</td>\n",
       "      <td>[(0, 6, B-TYPE), (7, 13, I-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24300</th>\n",
       "      <td>удобреия, подкорма</td>\n",
       "      <td>[(0, 9, O), (10, 18, O)]</td>\n",
       "      <td>[(0, 9, B-TYPE), (10, 18, I-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25438</th>\n",
       "      <td>хрусteam</td>\n",
       "      <td>[(0, 8, B-BRAND)]</td>\n",
       "      <td>[(0, 8, B-TYPE)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text  \\\n",
       "22823         сырокопченя кобаса   \n",
       "16163      перец черный горошком   \n",
       "11260               кэннон труcы   \n",
       "17625      приправа хмели сунели   \n",
       "13584  мороженое фруктовый лед !   \n",
       "14559                       нпро   \n",
       "23398         телятины охлжденне   \n",
       "10756              кртки, вероки   \n",
       "24300         удобреия, подкорма   \n",
       "25438                   хрусteam   \n",
       "\n",
       "                                                  spansl  \\\n",
       "22823                          [(0, 11, O), (12, 18, O)]   \n",
       "16163     [(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, O)]   \n",
       "11260                 [(0, 6, B-BRAND), (7, 12, B-TYPE)]   \n",
       "17625     [(0, 8, B-TYPE), (9, 14, I-TYPE), (15, 21, O)]   \n",
       "13584  [(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, O)...   \n",
       "14559                                        [(0, 4, O)]   \n",
       "23398                            [(0, 8, O), (9, 18, O)]   \n",
       "10756                            [(0, 6, O), (7, 13, O)]   \n",
       "24300                           [(0, 9, O), (10, 18, O)]   \n",
       "25438                                  [(0, 8, B-BRAND)]   \n",
       "\n",
       "                                             pred_spansl  \n",
       "22823               [(0, 11, B-TYPE), (12, 18, B-BRAND)]  \n",
       "16163  [(0, 5, B-TYPE), (6, 12, I-TYPE), (13, 21, I-T...  \n",
       "11260                [(0, 6, B-BRAND), (7, 12, B-BRAND)]  \n",
       "17625  [(0, 8, B-TYPE), (9, 14, B-BRAND), (15, 21, I-...  \n",
       "13584  [(0, 9, B-TYPE), (10, 19, I-TYPE), (20, 23, I-...  \n",
       "14559                                   [(0, 4, B-TYPE)]  \n",
       "23398                 [(0, 8, B-TYPE), (9, 18, B-BRAND)]  \n",
       "10756                  [(0, 6, B-TYPE), (7, 13, I-TYPE)]  \n",
       "24300                 [(0, 9, B-TYPE), (10, 18, I-TYPE)]  \n",
       "25438                                   [(0, 8, B-TYPE)]  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['spansl'] != test_df['pred_spansl']].sample(10)[['text', 'spansl', 'pred_spansl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1f61470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'B-TYPE': 22183,\n",
       "         'B-BRAND': 6523,\n",
       "         'O': 4820,\n",
       "         'I-TYPE': 4109,\n",
       "         'I-BRAND': 438,\n",
       "         'B-PERCENT': 142,\n",
       "         'B-VOLUME': 53,\n",
       "         'I-VOLUME': 25,\n",
       "         'I-PERCENT': 22})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "c = Counter(chain(*[[s['label'] for s in spans] for spans in train_df['spans'].values]))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b6aa057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.8676535272656256\n"
     ]
    }
   ],
   "source": [
    "score = compute_macro_f1(text_true_bio, [[s[1] for s in spans] for spans in text_pred_bio])\n",
    "print(\"Macro F1:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73bd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df['text'].tolist()\n",
    "test_spans = [\n",
    "    [(s['start'], s['end'], s['label']) for s in spans]\n",
    "    for spans in test_df['spans'].tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5cd60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
